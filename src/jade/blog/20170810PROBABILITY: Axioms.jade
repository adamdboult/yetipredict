extends ../templates/blogHeader.jade

block blogContent
	h2
		b Probability axioms
	p 10th March 2017
	p We have a set of possible outcomes and we have a function on the set to give the probability of each outcome.
	p Normally capital letters refers to predicates. Here \(P(x)\) refers to a function on \(x\).

	h3
		b Axioms
	
	p
		b Axiom 1 – events have probabilities
	p We define a set of events, where each event has an associated probability.
	p $$\forall x_i \in x, P(x_i)=c, (c\in \mathbb{R}\land 0\le c\le 1)$$
	p
		b Axiom 2 – outcomes of events are mutually exclusive
	p The sum of all of these probabilities is 1.
	p $$\sum_i P(x_i)=1$$
	p 
		b Axiom 3 – joint probabilities of events divide each event
	p We can assign a number to groups of outcomes, including from different probability spaces. The sum of these across one event space is equal to the probability of the remaining events.
	p $$\sum_j P(x_i \land y_j)=P(x_i)$$
	p This also applies to more than two events.
	p $$\sum_k P(x_i \land y_j \land z_k)=P(x_i \land y_j)$$
	p
		b Axiom 4 – joint probabilities of the same outcome are the probability of the outcome
	p The joint probability of the same event occurring is equal to the probability of the event occurring.
	p $$P(x_i \land x_i)=P(x_i)$$
	p
		b Axiom 5 - Events are mutually exclusive.
	p The probability of two outcomes in the same event occurring is \(0\).
	p $$P(x_i \land x_j)=0$$

	h3
		b Extra definitions
	p
		b OR
	p $$P(x_i \lor y_j):=P(x_i)+P(y_j)-P(x_i \land y_j)$$
	p
		b Conditional probability
	p $$P(x_i|y_j):=\frac{P(x_i \land y_j)}{P(y_j)}$$


	h3
		b Moments and expected values
	p
		b Expected value
	p For a random variable (or vector of random variables), \(x\), we define the expected value of \(f(x)\) as :
	p $$E[f(x)]:=\sum f(x_i) P(x_i)$$
	p The expected value of random variable \(x\) is therefore this where \(f(x)=x\).
	p $$E(x)=\sum_i x_i P(x_i)$$
	p We can also take moments of multiple random variables. For this we see \(f(x)\) as a function on a vector \(x\), and \(P(x_i)\) as the probability of a specific vector appearing:
	p $$E[f(x,y)]=\sum_i \sum_j f(x_i,y_j) P(x_i \land y_j)$$
	p
		b Properties of expected values
	p We can show that \(E(x+y)=E(x)+E(y)\):
	p $$E[x+y]=\sum_i \sum_j (x_i+y_j) P(x_i \land y_j)$$
	p $$E[x+y]=\sum_i \sum_j x_i [P(x_i \land y_j)]+\sum_i \sum_j [y_j P(x_i \land y_j)]$$
	p $$E[x+y]=\sum_i x_i \sum_j [P(x_i \land y_j)]+\sum_j y_j \sum_i [P(x_i \land y_j)]$$
	p $$E[x+y]=\sum_i x_i P(x_i)+\sum_j y_j P(y_j) $$
	p $$E[x+y]=E[x]+E[y]$$
	p
		b Moments
	p The \(n\)th moment of event \(x\) around \(c\) is defined as this where \(f(x)=(x-c)^n\)
	p $$E[(x-c)^n]=\sum_i (x_i-c)^n P(x_i)$$
	p The mean is the first moment around \(0\).
	p The variance is the second moment around the mean.
	p Variance and e(x)^2, e(x^2)
	p Var(x+y)=



	h3
		b Distributions
	p
		b Independent and identically distributed (IID): Independence
	p \(x\) is independent of \(y\) if:
	p $$\forall x_i \in x,\forall y_j \in y (P(x_i|y_j)=P(x_i)$$
	p If \(P(x_i|y_j)=P(x_i)\) then:
	p $$P(x_i\land y_j)=P(x_i).P(y_j)$$
	p This logic extends beyond just two events. If the events are independent then:
	p $$P(x_i\land y_j \land z_j)=P(x_i).P(y_j \land z_k)=P(x_i).P(y_j).P(z_k)$$
	p
		b Independent and identically distributed (IID): Identically distributed
	p \(x\) is identically distributed to \(y\) if:
	p $$\forall i (\exists x_i \rightarrow P(x_i)=P(y_i))$$
	h3
		b Examples of distributions
	p We know:
	p $$\sum_i P(x_i)=1$$
	p This can be satisfied in many ways:
	ul
		li Only one possible outcome, with certainty
		li Six outcomes, equally likely
		li Two sides of a coin, but biased
		li The normal distribution
	p
		b Cumulative distributions
	p
		bExamples of joint distributions
	p These could include:
	ul
		li Rolling a die then flipping a coin
		li Flipping a coin, and then reading it again
		li Flipping a coin and then reading the other side
	p In the first example, these are independent, so we can model:
	p $$P(x_i \land y_j)=P(x_i).P(y_j)$$
	p In the second these are the same measurement so:
	p $$P(x_i \land y_j)=P(x_i \land x_i)=P(x_i)$$
	p The third example is mutually exclusive so:
	p $$P(x_i \land y_j)=P(x_i \land x_{¬j})$$
	p Which is either \(0\) or \(P(x_i)\) depending on \(j\).
	p We can try more complicated examples, for example flipping the same coin multiple times.
	p $$P(x_i \land y_j \land z_k …)=P(x_i \land x_j \land x_k …)=P(x_i).P(x_j).P(x_z)…$$


	h3
		b Some theorems
	p
		b Markov’s inequality
	p
		b Chebyshev's inequality


	h3 Bayes theorem
	p
		b Bayes theorem
	p We know:
	p $$P(x_i|y_j):=\frac{P(x_i \land y_j)}{P(y_j)}$$
	p $$P(y_j|x_i):=\frac{P(x_i \land y_j)}{P(x_i)}$$
	p So:
	p $$P(x_i|y_j)P(y_j)=P(y_j|x_i) P(x_i)$$
	p $$P(x_i|y_j)=\frac{P(y_j|x_i) P(x_i)}{P(y_j)}$$
	p
		b Bayesian updating



	h3
		b Samples
	p
		b Sample set
	p We only observe the sample set \(S\), drawn from \(n\) random event spaces, not the probability functions themselves. It is the nature of the probability functions we are interested in.
	p For example we could observe a sample set from flipping a coin of:
	p $$\{H,T,T,H\}$$
	p We could also have more complex sets. For example if you were analysing temperature and time of day this could look like:
	p $$\{24,13:00,17,14:00, 19,15:00\}$$
	p
		b Sample measures
	p We can take measures on the sample and probability. These can enable us to estimate probability measures.
	p We previously discussed taking measures on the population distribution through calculating moments. 
	p We can also take measures on the sample set. Options include the sample size, the sample mean and the sample variance.
	p $$n:=count(x_i)$$
	p We can use similar thing to moments. The \(m\)th sample moment of \(x\) around \(c\) is:
	p $$\:=\frac{\sum_i (x_i-c)^m}{n}$$
	p $$\bar x :=\frac{\sum_i x_i}{n}$$
	p $$s^2:=\frac{\sum_i (x_i - \bar x)^2}{n}$$
	p E(\bar x)=E(x)
	p E(s^2)/=E(x-mew)^2
	p Weak law of large numbers
	p Let’s compare the sample average to the expected value. The sample average is:
	p $$\bar x =\frac{\sum_j x_j}{n}$$
	p The expected value is:
	p $$E(x)=\sum_i x_i P(x_i)$$
	p If \(q_i\) is the number of observations for that possible outcome, we know that:
	p $$\sum_j x_j =\sum_i x_i q_i$$
	p And so:
	p $$\bar x =\frac{\sum_j x_j}{n}=\frac{\sum_i x_i q_i}{n}$$
	p $$\bar x =\frac{\sum_j x_j}{n}=\sum_i x_i \frac{q_i}{n}$$
	p What happens to \(\frac{q_i}{n}\) as \(n\) increases?
	p Population estimates from samples
	p As n to infinity, looks like population

	p Small sample best estimate of population moments, has error ranges

	p Other sample stuff
	p Strong law of large numbers

	p Central limit theorem





	h3
		b Maximum Likelihood estimation
	p We often want to use sample data to estimate probability values. For example we could estimate:
	p $$P(head on next coin flip)$$
	p $$P(temperature is 20 | time of day is 13:00)$$
	p Let’s take our coin flips. The chance of that outcome was:
	p $$P(H_1 \land T_2 \land T_3 \land H_4)$$
	p More generally this is:
	p $$P(A_1 \land B_2 \land C_3 \land D_4…)$$
	p By considering what parameter maximise the chance of observing the data we see, and making assumptions on the data, we can estimate probability values.
	p
		b Assumption 1: Independence
	p If the events are independent, that is the chance of a flip doesn’t depend on any other outcomes, then:
	p $$P(A_1 \land B_2 \land C_3 \land D_4…)= P(A_1).P(B_2).P(C_3).P(D_4)…$$
	p
		b Assumption 2: Identically distributed
	p If the events are identically distributed, the chance of flipping a head doesn’t change across flips (for example the heads side doesn’t get heavier over time) then:
	p $$P(A_1).P(B_2).P(C_3).P(D_4)…= P(A).P(B).P(C).P(D)…$$
	p These two conditions mean the distribution is independent and identically distributed (iid).
	p
		b Maximisation
	p We want to find parameters to maximise this. We can take logarithms, which preserve stationary points. As logarithms are defined on all values above \(0\), and all probabilities are also above zero (or zero), this preserves solutions.
	p The non-zero stationary points of:
	p $$P(A).P(B).P(C).P(D)…$$
	p Are the same as those for:
	p $$ln(P(A))+ln(P(B))+ln(P(C))+ln(P(D))+…$$
	p Parameters
	p So we want to find parameters which maximise the above. But what are the parameters? For this we need a model.
	p Example 1: Coin Flip
	p Let’s take our simple example about coins. Heads and tails are the only options, so \(P(H)+P(T)=1\). 
	p $$2ln(P(H))+2ln(1-P(H))$$
	p We now want to find the value of \(P(H)\) which maximises the above.
	p $$\frac{2}{\delta P(H)}=\frac{2}{P(H)}-\frac{2}{1-P(H)}$$
	p The stationary points are then:
	p $$\frac{1}{P(H)}-\frac{1}{1-P(H)}=0$$
	p $$P(H)=1-P(H)$$
	p So \(P(H)=\frac{1}{2}\) is the value which makes our observation most likely.
