extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Markov Decision Processes
block subSubContent
	h3
		b Markov Decision Process
	p We have a state space. Actions move from one state to another, probabilistically.
	p The MDP can be described as \((S,s_1,A,P,R)\)
	p Where there are
	ul
		li \(S\) - the state space
		li \(s_1\) - the initial state
		li \(A\) - the action space
		li \(P\) - the transition model
		li \(R\) - the reward distribution

	p \(P(s_{t+1}|H)=P(s_{t+1}|s_t=s, a_t=a)=P_{s,a}\)
	p \(E[r_t|H]=E[r_t|s_t=s, a_t=a]=R_{s,a}\)

	p We maximise the reward function using discounting.
	p \(E[\sum_{t=1}^\infty \gamma^{t-1}r_t|s_1]\)

	p \(R_{s,a}\) and \(P_{s,a}\) are unknown.

