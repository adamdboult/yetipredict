extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Markov Decision Processes
block subSubContent
	h3
		b Markov Decision Process
	p We have a state space. Actions move from one state to another, probabilistically.
	p The MDP can be described as \((S,s_1,A,P,R)\)
	p Where there are
	ul
		li \(S\) - the state space
		li \(s_1\) - the initial state
		li \(A\) - the action space
		li \(P\) - the transition model
		li \(R\) - the reward distribution

	p \(P(s_{t+1}|H)=P(s_{t+1}|s_t=s, a_t=a)=P_{s,a}\)
	p \(E[r_t|H]=E[r_t|s_t=s, a_t=a]=R_{s,a}\)

	p We maximise the reward function using discounting.
	p \(E[\sum_{t=1}^\infty \gamma^{t-1}r_t|s_1]\)

	p \(R_{s,a}\) and \(P_{s,a}\) are unknown.
	h3
		b Policy
	p A policy maps the state onto the action
	p \(a_t=\pi (s_t)\)
	p The policy does not need to change over time, as discounting is constant. That is, if the policy should be different in future, it should be different now.
	p The policy affects the transition model, and so we have \(P_\pi \).

	h3
		b Value function
	p To evaluate a policy we use the value function. This is the expected return from following the policy.
	p We can calculate this by working through the different possible outcomes in each time period.
	p \(v_\pi = E[\sum_{t=1}^\infty \gamma^{t-1}r_t|s_1]\)

	h3
		b Bellman equations
	p We breakdown the value function into an immediate reward, and the discounted value function of the next state.
	p This is because the expectation function is linear.
	p \(v_\pi (s)=R_{s,\pi(s)}+\gamma \sum_{s'}P_{s,\pi(s)}(s')v_\pi (s')\)

	p We can write this in matrix form.
	p \(v_pi (s)= r_\pi + \gamma P_\pi v_\pi(s)\)

	p We can then solve this:
	p \(v_pi (s)= (I-\gamma P_\pi)^{-1})r_\pi\)

	p This depends on the starting state.

	h3
		b Optimal policy
	p There exists a policy that is better than any other policy, under any starting state.
	p There is no closed form solution to finding the optimal policy.
	p There are instead iterative methods.

	h3
		b Policy iteration method
	p We start with a random policy.
	p We then loop:
	ul
		li Evaluate the policy, using the Bellman equations.
		li Update the policy

	p We update the policy by changing \(a\) to maximise:
	p \(v_pi ' (s)= r_\pi + \gamma P_\pi v_\pi(s)\) 
	p For example, if we have a policy of doing \(a\) in state \(s\), we would see if we increase the value if we change \(a\) to \(a'\).


	h3
		b Value iteration method
	p Doesn't directly calculuate policy. Doesn't require inverting matrix

	h3
		b Q-learning

	p We can't use value functions, so we use Q-values instead.
	p \(Q(s,a)\) is the value of taking action \(a\) in state \(s\).
	p \(Q(s,a)=R_{s,a}+\gamma E_{s'}[max_{a'}Q(s',a')]\).

