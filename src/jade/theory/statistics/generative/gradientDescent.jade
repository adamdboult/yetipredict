extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Gradient descent
block subSubContent
	h3
		b Introduction
	p There are \(3\) main algorithms for gradient descent:
	ul
		li Batch gradient descent
		li Stochastic gradient descent
		li Mini-batch gradient descent
	h3
		b Batch gradient descent
	p Do gradient descent on all samples
	p The standard gradient descent algorithm above is also known as batch gradient descent. There are other implementations.

	h3
		b Stochastic gradient descent
	p Do gradient descent on one (?!) sample only

	p not guaranteed for each step to go towards minimum, but each step much faster
	h3
		b Mini batch gradient descent
	p use b samples on each iteration, b is parameter, between stochastic and batch
	p b=2-100 for example




	h3
		b Epochs
	p This refers to the number of times the whole dataset has been run.
	p 
	h3
		b Checking
	p We can use a slower alternative to back propagation to ensure that the algorithm is working correctly.
	p Gradient checking is used to manually work out the gradient of the cost function.
	p We can do this by taking a value of theta, adding a small value, \(\epsilon \), to and from it to get \(\theta +\) and \(\theta -\).
	p We can then calculate the gradient for \(\theta_i\) by calculating:
	p \(\frac{J(\theta +)-J(\theta -)}{2\epsilon }\)
