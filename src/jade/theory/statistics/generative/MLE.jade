extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Maximum likelihood estimation
block subSubContent
	h3
		b Maximising the likelihood function
	p We have a likelihood function of the data.
	p \(L(\theta ; X)=P(X|\theta )\)

	p We choose values for \(\theta \) which maximise the likelihood function.
	p \(argmax_\theta P(X|\theta )\)

	p That is, for which values  of \(\theta \) was the observation we saw most likely?
	p This is a mode estimate.
	h3
		b IID
	p \(L(\theta ; X)=\prod_i P(x_i|\theta )\)
	h3
		b Logarithms
	p We can take logarithms, which preserve stationary points. As logarithms are defined on all values above \(0\), and all probabilities are also above zero (or zero), this preserves solutions.
	p The non-zero stationary points of:
	p \(\ln L(\theta ; X)=\ln \prod_i P(x_i|\theta )\)
	p \(\ln L(\theta ; X)=\sum_i \ln P(x_i|\theta )\)
	h3
		b Example: Coin flip
	p Letâ€™s take our simple example about coins. Heads and tails are the only options, so \(P(H)+P(T)=1\). 
	p \(P(H|\theta )=\theta \)
	p \(P(T|\theta )=1-\theta \)
	p \(\ln L(\theta ; X)=\sum_i \ln P(x_i|\theta )\)
	p If we had \(5\) heads and \(5\) tails we would have:
	p \(\ln L(\theta ; X)=5\ln (\theta )+ 5\ln (1-\theta )\)
	p So \(P(H)=\frac{1}{2}\) is the value which makes our observation most likely.
	
	h3
		b Relative likelihood ratios
	p A bit like confidence intervals
	
