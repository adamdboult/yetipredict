extends ../../templates/theoryHeader.jade

block theoryContent
	h2
		b Transformations
	h3
		b Summary
	p Cumulative probability function
	p $$F=\int_{-\inf }^\inf xP(x)$$
	p Moment generating function
	p $$F=\int_{-\inf }^\inf e^{tx}P(x)$$
	p Characteristic function
	p $$F=\int_{-\inf }^\inf e^{itx}P(x)$$
	h3
		b Cumulative probability function
	p We have a probability density function \(P(x)\). This is shorthand for \(P(X=x)\).
	p We may often want to instead show: \(P(X\ge x)\).
	p This is the same as
	p $$P(X\ge x)=P[(X=x) \lor (X=x+\delta )\lor …]$$
	p As these are mutually exclusive:
	p $$P(X\ge x)=P(X=x)+P(X=x+\delta )…$$
	p Then for the discrete case:
	p $$P(X\ge x)=\sum_{a=x}^\inf P(a)$$
	p For the continuous case:
	p $$P(X\ge x)=\int _{a=x}^\inf P(a)$$
	p Note that:
	p $$P[(X \ge x) \lor (X< x)]=P(\omega )=1$$
	p $$P[(X \ge x) \lor (X< x)]=P(X\ge x)+P(X<x)$$
	p So:
	p $$P(X\ge x)+P(X<x)=1$$
	p $$P(X> x)+P(X\le x)=1$$

	p We may also want to show the probability that a variable is between two points.
	p $$P(a\le x\le b)$$
	p Note that:
	p $$P[(a\le x)\land (x\le b)]=P(a\le x)+P(x\le b)-P[(a\le x)\lor (x\le b)]$$
	p $$P(a \le x\le b)=P[(a\le x)\land (x\le b)]$$
	p So
	p $$P(a \le x\le b)=P(a\le x)+P(x\le b)-P[(a\le x)\lor (x\le b)]$$
	p If \(b>a\):
	p $$P[(a\le x)\lor (x\le b)]=1$$
	p $$P(a \le x\le b)=P(a\le x)+P(x\le b)-1$$
	p Else:
	p $$P[(a\le x)\lor (x\le b)]=P(a\le x)+P(x\le b)$$
	p $$P(a \le x\le b)=0$$
	h3
		b Moment generating function
	p Take random variable \(X\). This has moments we wish to calculate.
	p We can transform our function in other forms which maintain all of the required information. For example we could also use the cumulative probability function to calculate moments. We now look for an alternative form of the probability density function which allows us to easily calculate moments.
	p One method is to use the probability density function and the definitions of moments, but there are other options. For example, consider the function:
	p $$E[e^{tX}]$$
	p Which expands to:
	p $$E[e^{tX}]=\sum_{j=1}^\inf \frac{t^jE[X^j]}{j!}$$
	p By taking the \(m\)th derivative of this, we get
	p $$E[X^m]+\sum_{j=m+1}^\inf \frac{t^jE[X^j]}{j!}$$
	p We can then set \(t=0\) to get
	p $$E[X^m]$$
	p Alternatively, see that differentiating \(m\) times gets us
	p $$E[X^me^{tX}]$$
	p If we can get this function, we can then easily generate moments.
	p The function we need to get is:
	p $$E[e^{tX}]$$
	p In the discrete case this is:
	p $$E[e^{tX}]=\sum_{i=1}e^{tx_i}p_i$$
	p In the continuous case:
	p $$E[e^{tX}]=\int_{-\inf }^\inf e^{tx}P(x) dx$$
	h3
		b Characteristic function
	p It may not be possible to calculate the integral for the moment generating function. We now look for an alternative formula with which we can generate the same moments.
	p Consider
	p $$E[e^{itX}]$$
	p As this can be broken down into sinusoidal functions it can more readily be integrated.
	p This expands to
	p $$E[e^{itX}]=\sum_{j=1}^\inf \frac{i^jt^jE[X^j]}{j!}$$
	p By taking the \(m\)th derivative we get.
	p $$E[X^m]i^m+\sum_{j=m+1}^\inf \frac{t^jE[X^j]}{j!}$$
	p By setting \(t=0\) we then get:
	p $$E[X^m]i^m$$
	p Alternatively see that differentiating \(m\) times gets us
	p $$E[(iX)^me^{itX}]
	p So we can get the moment by differentiating \(m\) times, and multiplying by \(i^{-m}\).
