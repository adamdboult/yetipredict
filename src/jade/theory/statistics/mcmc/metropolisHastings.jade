extends ./subSubTemplate.jade
block subSubTitle
	h1
		b The Metropolis-Hastings algorithm
block subSubContent
	h3
		b Density estimation through direct sampling

	p There is distribution \(P(x)\) which we want to know more about.

	p If the function was closed, we could estimate it by using values of \(x\).

	h3
		b Limitations of direct sampling

	p However if the function does not have such a form, we cannot do that.

	p We can't plug in values, because the function is complex.

	p Sometimes we may know a function of the form:

	p \(f(x)=cP(x)\)

	p That is, a multiple of the function.

	p This can happen from Bayes' theorem:

	p \(P(y|x)=\frac{P(x|y)P(y)}{P(x)}\)

	p We may be able to estimate \(P(x|y)\) and \(P(y)\), but not \(P(x)\)

	p This means be have 

	p \(P(y|x)=cP(x|y)P(y)\)

	h3
		b The Metropolis-Hastings algorithm
	p The Metropolis-Hastings algorithm creates a set of samples \(x\) such that the distribution of the samples approaches the goal distribution.

	p The algorithm takes an arbitrary starting sample \(x_0\). It then must decide which sample to consider next.

	p It does this using a Markov chain. That is, there is a map \(g(x_j, x_i)\).

	p This distribution is generally a normal distribution around \(x_i\), making the process a random walk.

 

	p Now we have a considered sample, we can either accept or reject it. It is this step that makes the end distribution approximage the function.

	p We accept if \(\frac{f(x_j)}{f(x_i)}>u\), where \(u\) is a random variable between \(0\) and \(1\), generated each time.

	p We can calculate this because we know this function.

