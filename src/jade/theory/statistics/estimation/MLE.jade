extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Maximum likelihood estimation
block subSubContent
	p We often want to use sample data to estimate probability values. For example we could estimate:
	p $$P(head on next coin flip)$$
	p $$P(temperature is 20 | time of day is 13:00)$$
	p Let’s take our coin flips. The chance of that outcome was:
	p $$P(H_1 \land T_2 \land T_3 \land H_4)$$
	p More generally this is:
	p $$P(A_1 \land B_2 \land C_3 \land D_4…)$$
	p By considering what parameter maximise the chance of observing the data we see, and making assumptions on the data, we can estimate probability values.
	p
		b Assumption 1: Independence
	p If the events are independent, that is the chance of a flip doesn’t depend on any other outcomes, then:
	p $$P(A_1 \land B_2 \land C_3 \land D_4…)= P(A_1).P(B_2).P(C_3).P(D_4)…$$
	p
		b Assumption 2: Identically distributed
	p If the events are identically distributed, the chance of flipping a head doesn’t change across flips (for example the heads side doesn’t get heavier over time) then:
	p $$P(A_1).P(B_2).P(C_3).P(D_4)…= P(A).P(B).P(C).P(D)…$$
	p These two conditions mean the distribution is independent and identically distributed (iid).
	p
		b Maximisation
	p We want to find parameters to maximise this. We can take logarithms, which preserve stationary points. As logarithms are defined on all values above \(0\), and all probabilities are also above zero (or zero), this preserves solutions.
	p The non-zero stationary points of:
	p $$P(A).P(B).P(C).P(D)…$$
	p Are the same as those for:
	p $$ln(P(A))+ln(P(B))+ln(P(C))+ln(P(D))+…$$
	p Parameters
	p So we want to find parameters which maximise the above. But what are the parameters? For this we need a model.
	p Example 1: Coin Flip
	p Let’s take our simple example about coins. Heads and tails are the only options, so \(P(H)+P(T)=1\). 
	p $$2ln(P(H))+2ln(1-P(H))$$
	p We now want to find the value of \(P(H)\) which maximises the above.
	p $$\frac{2}{\delta P(H)}=\frac{2}{P(H)}-\frac{2}{1-P(H)}$$
	p The stationary points are then:
	p $$\frac{1}{P(H)}-\frac{1}{1-P(H)}=0$$
	p $$P(H)=1-P(H)$$
	p So \(P(H)=\frac{1}{2}\) is the value which makes our observation most likely.
	
	h3
		b Definition
	p Mode estimate
	p \(Arg max_\theta \theta p(X|\theta )\)
	p If X is independnet then
	p \(Arg max_\theta \theta p(X|\theta )= arg max_\theta \prod p(x|\theta )\)
	p We can do a monotonic transformation
	p \(Arg max_\theta \theta p(X|\theta )= arg max_\theta \sum \ln p(x|\theta )\)
	h3
		b Relative likelihood ratios
	p A bit like confidence intervals
	h3
		b Other
	p Arg max _\theta p(X|theta)

	p Mode estimate

	p If X is independent then

	p Arg max _\theta p(X|theta)= Arg max _\theta \prod p(x|theta)

 

	p Can do monotonic transform

	p Arg max _\theta p(X|theta)= Arg max _\theta \sum \ln p(x|theta)
	
