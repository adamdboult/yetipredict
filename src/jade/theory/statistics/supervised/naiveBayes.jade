extends ./subSubTemplate.jade
block subSubTitle
	h1
		b The Naive Bayes classifier
block subSubContent
	h3
		b Calculating the Naive Bayes estimator
	p With the Naive Bayes assumption we have:
	p \(P(y|x_1,x_2,...,x_n)\propto P(x_1|y)P(x_2|y)...P(x_n|y)P(y)\)
	p We now choose \(y\) which maximises this.
	p This is easier to calculate, as there is less of a sample restriction.
	p This is used when evidence is also in classes, as the chance of any individual outcome on a continuous probability is \(0\).
	h3
		b Estimating \(P(y)\)
	p We can easily calculate \(P(y)\), by looking at the frequency across the sample.
	h3
		b Estimating \(P(x_1|y)\)
	p Normally, \(P(x_1|y)=\frac{n_c}{n_y}\), where:
	ul
		li \(n_c\) is the number of instances where the evidence is \(c\) and the label is \(y\).
		li \(n_y\) is the number of instances where the label is \(y\).
	h3
		b Regularising the Naive Bayes estimator
	p To reduce the risk of specific probabilities being zero, we can adjust them, so that:
	p \(P(x_1|y)=\frac{n_c+mp}{n_y+m}\), where:
	ul
		li \(p\) is the prior probability. If this is unknown, use \(\frac{1}{k}\), where \(k\) is the number of classes.
		li \(m\) is a parameter called the equivilant sample size.

