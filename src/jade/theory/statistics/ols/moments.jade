extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Expectation of OLS estimators
block subSubContent
	h3
		b Expectation in terms of observables
	p We have: \(\hat{\theta }=(X^TX)^{-1}X^Ty\)
	p Let’s take the expectation.
	p \(E[\hat{\theta }]=E[(X^TX)^{-1}X^Ty]\)
	h3
		b Expectation in terms of errors
	p Let’s model \(y\) as a function of \(X\). As we place no restrictions on the error terms, this is not as assumption.
	p \(y=X\theta +\epsilon\). 
	p \(E[\hat{\theta }]=E[(X^TX)^{-1}X^T(X\theta +\epsilon)]\)
	p \(E[\hat{\theta }]=E[(X^TX)^{-1}X^TX\theta ]+E[(X^TX)^{-1}X^T \epsilon)]\)
	p \(E[\hat{\theta }]=\theta +E[(X^TX)^{-1}X^T \epsilon)]\)
	p \(E[\hat{\theta }]=\theta +E[(X^TX)^{-1}X^T]E[ \epsilon]+cov [(X^TX)^{-1}X^T ,\epsilon]\)
	h3
		b The Gauss-Markov: Expected error is \(0\)
	p \(E[\epsilon  =0]\)
	p This means that:
	p \(E[\hat{\theta }]=\theta + cov [(X^TX)^{-1}X^T ,\epsilon]\)


	h3
		b The Gauss-Markov: Errors and indepedent variables are uncorrelated
	p If the error terms and \(X\) are uncorrelated then \(E[\epsilon|X]=0\) and therefore:
	p \(E[\hat{\theta }]=\theta\)
	p So this is an unbiased estimator, so long as the condition holds.



