extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Normal equation
block subSubContent
	h3
		b Least squares
	p The square error is \(\sum_i (\hat{y_i}-y_i)^2\).
	p The differential of this with respect to \(\hat{\theta_j }\) is:
	p \(2\sum_i \frac{\delta \hat{y_i}}{\delta \hat{\theta_j}}(\hat{y_i}-y_i)\)
	p The stationary point is where this is zero:
	p \(\sum_i \frac{\delta \hat{y_i}}{\delta \hat{\theta_j}}(\hat{y_i}-y_i)=0\)


	h3
		b Linear least squares
	p Here, \(\hat{y_i}= \sum_j x_{ij}\hat{\theta_j}\)
	p Therefore: \(\frac{\delta \hat{y_i}}{\delta \hat{\theta_j}}=x_{ij}\)
	p And so the stationary point is where
	p \(\sum_i x_{ij}( \sum_j x_{ij}\hat{\theta_j }-y_i)=0\)
	p \(\sum_i x_{ij}( \sum_j x_{ij}\hat{\theta_j)}= \sum_i x_{ij}y_i\)


	h3
		b Normal equation
	p We can write this in matrix form.
	p \(X^TX\hat{\theta }=X^Ty\)
	p We can solve this as:
	p \(\hat{\theta }=(X^TX)^{-1}X^Ty\)

	h3
		b Moore-Penrose pseudoinverse
	p For a matrix \(X\), the pseudoinverse is \((X^*X)^{-1}X^*\).
	p For real matrices, this is: \((X^TX)^{-1}X^T\)
	p The pseudoinverse can be written as \(X^+\)
	p Therefore \(\theta \) is the pseudoinverse of the inputs, multiplied by the outputs. Or:
	p \(\theta = X^+y\)
	p The pseudoinverse satisfies:
	p \(XX^+X=X\)
	p \(X^+XX^+=X^+\)

	h3
		b Perfectly correlated variables
	p If variables are perfectly correlated then we cannot solve the normal equation.
	p Intuitively, this is because for perfectly correlated variables there is no single best parameter, as changes to one parameter can be counteracted by changes to another.

