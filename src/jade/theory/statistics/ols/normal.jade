extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Normal equation
block subSubContent
	h3
		b Least squares
	p The square error is \(\sum_i (\hat{y_i}-y_i)^2\).
	p The differential of this with respect to \(\hat{\theta_j }\) is:
	p \(2\sum_i \frac{\delta \hat{y_i}}{\delta \hat{\theta_j}}(\hat{y_i}-y_i)\)
	p The stationary point is where this is zero:
	p \(\sum_i \frac{\delta \hat{y_i}}{\delta \hat{\theta_j}}(\hat{y_i}-y_i)=0\)
	h3
		b Linear least squares
	p Here, \(\hat{y_i}= \sum_j x_{ij}\hat{\theta_j}\)
	p Therefore: \(\frac{\delta \hat{y_i}}{\delta \hat{\theta_j}}=x_{ij}\)
	p And so the stationary point is where
	p \(\sum_i x_{ij}( \sum_j x_{ij}\hat{\theta_j }-y_i)=0\)
	p \(\sum_i x_{ij}( \sum_j x_{ij}\hat{\theta_j)}= \sum_i x_{ij}y_i\)
	h3
		b Normal equation
	p We can write this in matrix form.
	p \(X^TX\hat{\theta }=X^Ty\)
	p We can solve this as:
	p \(\hat{\theta }=(X^TX)^{-1}X^Ty\)
	h3
		b The projection matrix
	p We have \(X\).
	p The projection matrix is \(X(X^TX)^{-1}X^T\)
	p The projection matrix maps from actual y to predicted y
	p \(\hat y = Py\)
	p Each entry refers to the covariance between actual and fitted

	p \(p_{ij}=\frac{\cov (\hat y_i, y_j}{\var (y_j)}\)
	h3
		b The annihilation matrix
	p We can get residuals too:

	p u=y-\hat y=y-py=(1-P)y

	p 1-P is called the annihilator matrix
	p We can now use the propagation of uncertainty

	p \(\Sigma^f = A\Sigma^x A^T\)

	p To get:

	p \(\Sigma^u = (I-P)\Sigma^y (I-P)\)

	p Annihilator matrix is:

	p \(M_X=I-X(X^TX)^{-1}X^T\)

	p Called this because:

	p \(M_XX=X-X(X^TX)^{-1}X^TX\)

	p \(M_XX=0\)

	p 
	p Is called residual maker

	p My=\epsilon
 
	h3
		b Moore-Penrose pseudoinverse
	p For a matrix \(X\), the pseudoinverse is \((X^*X)^{-1}X^*\).
	p For real matrices, this is: \((X^TX)^{-1}X^T\)
	p The pseudoinverse can be written as \(X^+\)
	p Therefore \(\theta \) is the pseudoinverse of the inputs, multiplied by the outputs. Or:
	p \(\theta = X^+y\)
	p The pseudoinverse satisfies:
	p \(XX^+X=X\)
	p \(X^+XX^+=X^+\)

	h3
		b Perfectly correlated variables
	p If variables are perfectly correlated then we cannot solve the normal equation.
	p Intuitively, this is because for perfectly correlated variables there is no single best parameter, as changes to one parameter can be counteracted by changes to another.

