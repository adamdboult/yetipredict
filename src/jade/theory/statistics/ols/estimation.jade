extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Joint OLS
block subSubContent
	h3
		b Simple OLS
	p We have an independent variable, \(x\), and a dependent variable, \(y\). We want to draw a straight line through a plot of this data to find the best fit. We define the best fit as the line which minimises the sum of the squares of the difference between actual values of \(y\) and the position of our line for that point.
	p $$y_i=\ m.x_i +c+\epsilon_i$$
	p We want to minimise the sum of \(\epsilon^2\) so let’s rearrange.
	p $$\epsilon_i=y_i- m x_i –c$$
	p $$\epsilon_i^2=y_i^2 +c^2 +m^2 x_i^2 +2cmx_i -2cy_i -2mx_i y_i$$
	p $$\sum \epsilon_i^2=\sum y_i^2 +nc^2 +m^2\sum x_i^2+2cm\sum x_i -2c\sum y_i -2m \sum x_i y_i $$

	p We need to find the values of \(m\) and \(c\) which minimise this value. Let’s find the stationary point for \(\m\).

	p $$\frac{\delta \sum \epsilon_i^2}{\delta m}= 2m\sum x_i^2+2c\sum x_i-2 \sum x_i y_i=0$$
	p $$m\sum x_i^2=\sum x_i y_i -c\sum x_i $$
	p And the stationary point for \(c\).
	p $$\frac{\delta \sum \epsilon_i^2}{\delta c}= 2cn+2m\sum x_i -2\sum y_i=0$$
	p $$cn+m\sum x_i -\sum y_i=0$$
	p So we have:
	p $$m\sum x_i^2=\sum x_i y_i -c\sum x_i $$
	p $$cn+m\sum x_i -\sum y_i=0$$

	p We can rearrange these to:
	p $$c =\frac{\sum x_i y_i - m\sum x_i^2}{\sum x_i}$$
	p $$c=\frac{\sum y_i-m\sum x_i}{n}$$
	p We can then combine these to solve for \(m\):
	p $$frac{\sum x_i y_i - m\sum x_i^2}{\sum x_i}=\frac{\sum y_i-m\sum x_i}{n}$$
	p $$n\sum x_i y_i - mn\sum x_i^2=\sum x_i \sum y_i-m\sum x_i \sum x_i$$
	p $$m(\sum x_i \sum x_i - n\sum x_i^2)= \sum x_i \sum y_i - n\sum x_i y_i $$
	p $$m=\frac{\sum x_i \sum y_i - n\sum x_i y_i}{\sum x_i \sum x_i - n\sum x_i^2}$$
	p $$m=\frac{n\sum x_i y_i-\sum x_i \sum y_i}{n\sum x_i^2-\sum x_i \sum x_i}$$
	p $$m=\frac{\bar xy -\bar x \bar y}{\bar x^2-{\bar x}^2}$$
	p We can then substitute for \(c\):
	p $$c=\frac{\sum y_i-m\sum x_i}{n}$$
	p $$c=\bar y -m\bar x$$
	p $$c=\bar y -\frac{\bar xy -\bar x \bar y}{\bar x^2-{\bar x}^2}\bar x$$
	h3
		b OLS
	p We can add an array of explanatory variables, rather than just 1. In addition instead of treating c separately, we add an \(x\) value of one to the dataset, to the same effect.
	p The model is now:
	p $$y_i=\sum m_j x_ij +\epsilon_i$$
	p We can work out the value of the error term squared:
	p $$\epsilon_i=y_i-\sum m_j x_ij$$
	p $$\epsilon_i^2=(y_i-\sum m_j x_ij)^2$$
	p $$\epsilon_i^2=y_i^2 + (\sum m_j x_ij)^2 -2y_i \sum m_j x_ij$$
	p And the sum of the errors squared:
	p $$\sum_i \epsilon_i^2=\sum_i y_i^2 + \sum_i (\sum_j m_j x_ij)^2 -2\sum_i y_i \sum_j m_j x_ij$$
	p Let’s now take the derivative of this with respect to m_J:
	p $$\frac{\delta \sum_i \epsilon_i^2}{\delta m_J}=2\sum_i x_iJ(\sum_j m_j x_ij) -2\sum_i y_i x_iJ=0$$
	p $$\sum_i x_iJ(\sum_j m_j x_ij)=\sum_i y_i x_iJ$$
	p These give us simultaneous equations to solve. First we examine one of these equations, and then we solve with linear algebra.
	p $$\sum_i x_iJ(m_J x_iJ + \sum_{j\ne J} m_j x_ij)=\sum_i y_i x_iJ$$
	p $$\sum_i x^2_iJ m_J + \sum_i x_iJ\sum_{j\ne J} m_j x_ij)=\sum_i y_i x_iJ$$
	p $$ m_J =\frac{\sum_i y_i x_iJ- \sum_i x_iJ \sum_{j\ne J} m_j x_ij)}{sum_i x^2_iJ}$$
	p $$ m_J =\frac{\sum_i y_i x_iJ}{ sum_i x^2_iJ} –\sum_{j\ne J}\frac{\sum_i x_iJ m_j x_ij}{sum_i x^2_iJ}$$
	p We can interpret this as saying: we do not need to much as much weight on data series which are already correlated with others.
	p Take the case of the intercept, where \(x_{ic}=1\) for all \(i\).
	p $$ m_c =\frac{\sum_i y_i x_ic}{ sum_i x^2_ic} –\sum_{j\ne c}\frac{\sum_i  x_ic m_j x_ij}{sum_i x^2_ic}$$
	p $$ m_c =\frac{\sum_i y_i .1}{n} –\sum_{j\ne c}\frac{\sum_i .1. m_j x_ij}{n}$$
	p $$ m_c =\bar y –\sum_{j\ne c} m_j \bar x_j $$
	p We can see this is a generalisation of the simple OLS.
	p We can solve all values of \(m_j\) using linear algebra.
	p $$\sum_i x_iJ(\sum_j m_j x_ij)=\sum_i y_i x_iJ$$
	p We can write this as matrix multiplication.
	p $$x_J^TXM=x_J^T y$$
	p As this holds for all \(j\) we can generalise to the following.
	p $$X^{T}XM=X^T y$$
	p $$M=(X^{T}X)^{-1}X^T y$$
	p Note that this only has a solution if \(X^TX\) is invertible. Intuitively it isn’t if there are more variables than samples, or if variables are linearly dependent. Examples of linearly dependent variables include “distance in miles” and “distances in kilometres”.



	h3
		b Univariate linear regression
	p Get each theta, get the variance of each theta.

	p Of form y=mx+c, or
	p $$h_\theta (x)=\theta _0 + \theta _1 x$$

	p Know x, y for existing sample.

	p Estimates of $\theta _0$ and $\theta _1$ result in different accuracy.
	p Can use cost functions to measure fit of estimates.

	p Standard sum of square residuals fine. Framed as:
	p $$J(\theta _0 ,\theta _1 )=\frac{1}{2m}\sum\limits_{i=1}^m (h_\theta (x^{(i)})-y^{(i)})^2$$
	p theta = 1/2m sum(m) [h(theta) -y]^2

	p h refers to hypothesis

	p $$h_\theta (x)=\theta _0 +\theta _1 x=h(x)$$

	p Divided by 2 for easier for dealing with derivatives.

	p Can take derivates of cost function for estimate (ends with OLS...)

	p In practice derivatives may not be simple, can do gradient descent.
	p take starting point, go down steepest part.

	p not guaratneed to arrive at global minimum, depends on start point and type of regression. linear regression doesn't have this issue

	p $$\theta _j := \alpha \frac{\delta }{\delta \theta _j}J(\theta _0,\theta _1)$$

	p alpha sets rate of descent.

	p := used to denote an update of variable. Used in programming, eg x=x+1.

	p As more than one variable used, needs to be:
	p $$temp0:=\theta _0 -\alpha...$$
	p $$temp1:=\theta _1 -\alpha...$$
	p $$\theta _0=temp0$$
	p $$\theta _1=temp1$$

	p Prevents misestimates, temp1 needs to be based on old value of theta.

	p As derivative term gets smaller, convergance happens ("large" moves at start, then smaller steps)

	p Derivative term for least squares:
	p $$\frac{\delta }{\delta \theta _0}J(\theta _0,\theta _1)=\frac{1}{m}\sum\limit_{i=1}^m (h_{\theta} (x^{(i)})-y^{(i)})$$
	p $$\frac{\delta }{\delta \theta _1}J(\theta _0,\theta _1)=\frac{1}{m}\sum\limit_{i=1}^m (h_{\theta} (x^{(i)})-y^{(i)})x^{(i)}$$

	p Can plug into update equation

	p This is batch gradient descent. Each step of descent uses all training examples


	p theta 0 := theta 0 - alpha/m sum(h0(x) - y)
	p theta j := theta j - alpha/m sum(h0(x) - y)xj

	p Can use matricies to do all this stuff

	p eg linear regresssion can be shown as:

	p features for gradient descent should be roughly same scale
	p if rooms and size sqft, then can scale size downwards

	p makes trajectory smoother, gets there faster
	p get things to -1<x<1 range


	p get run for x iterations,evaluate j(theta)

	p can stop if not lowering by much
	p can check if j theta increasing, means bad methodology, lower alpha

	p can generate more varibles for polynomial stuff
	p x x2, log x etc, sqr x, then have y = x + log x etc

	p have implication for feature scaling


	p Design matrix
	p | 1 | 2104 |
	p | 1 | 1416 |
	p | 1 | 1534 |
	p | 1 |  852 |

	p times X

	p 1st col is all 1s, column for times by constant

	p |  -40 |
	p | 0.25 |



	p Where 2nd col is, say, house sizes in sqr feed, and -40 is theta_0, 0.25 is gradient.

	p Can have multiple columns for additional hypotheses


	p can get theta without estimation by calcing
	p (XtX)^-1 Xt y

	p This normal equation gets exact answer of theta
	p which to use?
	p normal needs no alpha, iterations, is exact

	p but gradient descent works better with v large number of features, unlike normal equation.calcuing x'x^-1 is n^3 complexity in software

	p anything above n = 10 000 is slow, gradient descent can be better


	p feature scaling:

	p [x- min(x)] / [max(x) min(x)]

	
