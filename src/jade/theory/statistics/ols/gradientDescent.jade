extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Gradient descent for linear regression
block subSubContent
	h3
		b Limitations of normal equation
	p The normal equation may not always be appropriate. Solving it can be computationally very expensive, especially if there are a large number of features.
	p At around \(10000\) features, gradient descent can become faster than the normal equation.

	p normal needs no alpha, iterations, is exact


	p Derivative term for least squares:
	p $$\frac{\delta }{\delta \theta _0}J(\theta _0,\theta _1)=\frac{1}{m}\sum\lim_{i=1}^m (h_{\theta} (x^{(i)})-y^{(i)})$$
	p $$\frac{\delta }{\delta \theta _1}J(\theta _0,\theta _1)=\frac{1}{m}\sum\lim_{i=1}^m (h_{\theta} (x^{(i)})-y^{(i)})x^{(i)}$$
	p This is batch gradient descent. Each step of descent uses all training examples


