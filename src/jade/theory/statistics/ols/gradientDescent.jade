extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Gradient descent
block subSubContent
	h3
		b Limitations of normal equation
	p The normal equation may not always be appropriate. Solving it can be computationally very expensive, especially if there are a large number of features.
	p At around \(10000\) features, gradient descent can become faster than the normal equation.

	p normal needs no alpha, iterations, is exact

	h3
		b What is gradient descent?
	p Rather than solve a normal equation, gradient descent takes the loss function, and takes the derivative of the loss function with respect to each parameter.
	p Small adjustments are then made to the parameters, in the direction of the steepest derivative, resulting in better parameters.

	p As derivative term gets smaller, convergance happens. The largest changes to the parametres occurs early on in the algorithm.


	p can stop if not lowering by much


	p Derivative term for least squares:
	p $$\frac{\delta }{\delta \theta _0}J(\theta _0,\theta _1)=\frac{1}{m}\sum\lim_{i=1}^m (h_{\theta} (x^{(i)})-y^{(i)})$$
	p $$\frac{\delta }{\delta \theta _1}J(\theta _0,\theta _1)=\frac{1}{m}\sum\lim_{i=1}^m (h_{\theta} (x^{(i)})-y^{(i)})x^{(i)}$$
	p This is batch gradient descent. Each step of descent uses all training examples

	p theta 0 := theta 0 - alpha/m sum(h0(x) - y)
	p theta j := theta j - alpha/m sum(h0(x) - y)xj

	p can check if j theta increasing, means bad methodology, lower alpha


	p get run for x iterations,evaluate j(theta)

	p Can use matrices to do each step
	p There are \(3\) main algorithms for gradient descent:
	ul
		li Batch gradient descent
		li Stochastic gradient descent
		li Mini-batch gradient descent
	h3
		b Gradient descent and feature scaling
	p makes trajectory smoother, gets there faster
	p get things to -1<x<1 range

	h3
		b Local minima
	p Gradient descent is not guaratneed to arrive at a global minimum. For some loss functions, there will be multiple local minima, and gradient descent can end up in the wrong one.
	p Linear regression does not have this issue.
	
	h3
		b Batch gradient descent
	p := used to denote an update of variable. Used in programming, eg x=x+1.
	p $$\theta _j := \alpha \frac{\delta }{\delta \theta _j}J(\theta _0,\theta _1)$$
	p \(\alpha \) sets rate of descent.


	h3
		b Stochastic gradient descent
	p The standard gradient descent algorithm above is also known as batch gradient descent. There are other implementations.

	h3
		b Mini-batch gradient descent

