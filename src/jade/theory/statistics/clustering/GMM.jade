extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Gaussian Mixture Models
block subSubContent
	h3
		b Mixture models
	p We have a latent variable which is part of the process

	p variable is distributed according to parametric distirbution, but parameters are different for differnt latent classes.

	p There are \(K\) latent classes, and so \(K\) sets of parameters.

	p The popualtion is weighted into the \(K\) classes.
	h3
		b Gaussian Mixture Models (GMM)
	p In a Gaussian Mixture Model each non latent variable has a normal distriubtion with a mean and variance. For multiple variables there is a covariance matrix.
	h3
		b Expectation-Maximisation algorithm
	p This is used to learn the parameters for a Gaussian Mixture Model

	p We cannot simply maximise the likelihood function, because this cannot be specified for a latent model.

	p The log likelihood function normally is:

	p \(L(\theta ; X)=p(X|\theta )\)

	p With hidden variables it is:

	p \(L(\theta ; X, Z)=p(X|\theta )=\int p(X, Z|\theta)dZ\)

	h3
		b 1: Expectation step

	p We consider the expected log likelihood. We call this 

	p \(E[\log L(\theta ; X, Z)]\)

	h3
		b 2: Maximisation step
	p 
