extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Pruning decision trees
block subSubContent
	h3
		b Introduction
	p Training a decision tree until there is only one entry from the training set will result in overfitting.
	p We can use pruning to regularise trees.
	h3
		b Pruning
	h3
		b Reduced error pruning
	p From bottom, replace each node with a leaf of the most popular class. Accept if no reduction in accuracy.
	h3
		b Cost complexity pruning
	p Take full tree \(T_0\)
	p Iteratively find a subtree to replace with a leaf. Cost function is accuracy and number of leaves.
	p Remove this generating \(T_{i+1}\)
	p When we have just the root, choose a single tree using CV.
	h3
		b Growing and pruning
	p Generally we would split the data up. Grow the tree with one set and then prune with the other.
	p We can split our data up and iterate between growing and pruning.
	p When pruning, for each pair of leaves we test to see if they should be merged.
	p If our two sets are \(A\) and \(B\) we can do:
	ul
		li \(A\): Grow
		li \(B\): Prune
		li \(B\): Grow
		li \(A\): Prune
	p And repeat this process.
	h3
		b Partial regression trees
	p Once we have built a tree, we keep a single leaf and disard the rest.

