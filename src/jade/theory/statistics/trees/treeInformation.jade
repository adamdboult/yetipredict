extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Training decision trees with information gain
block subSubContent
	h3
		b Introduction
	p We can train a decision tree by starting out with the most simple tree - all outcomes in same node.
	p We can then do a greedy search to identify which split on the node is best.
	p We can then iterate this process on future nodes.
	h3
		b Training with information gain
	p We split nodes to increase maximum entropy.
	p Entropy is:
	\(E= -\sum_i^n p_{i=1}\log_2 p_i\)
	p Where we are summing across all nodes.
	h3
		b Information gain
	p The gain in entropy is the original entropy - weighted by size entropy of each branch
	h3
		b Information gain ratio

