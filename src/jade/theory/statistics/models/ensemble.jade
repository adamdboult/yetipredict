extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Ensemble models
block subSubContent
	h3
		b Majority voting
	p If there are multiple ways of classifying data, we can use multiple models, and using voting.
	p As each individual model can have unique information, this works better than one individual model.
	h3
		b Condorcet's Jury Theorem
	p If voters are independent, and the chance of one vote being right is greater than \(0.5\), then the more voters, the better.
	p A weak model can still be useful, if it is independent.
	h3
		b Bootstrapping
	p Bootstraping takes samples from the raw data set and uses these for estimators.		
	h3
		b Boosting
	p Boosting uses bootstrapping.
	p In boosting, we create independent weak models by:
	ul
		li Creating a weak model using the training data \(G_1(x)\).
		li Creating a new weighting for the dataset, where ones poorly predicted are given high weights.
		li Creating another weak model on this data set \(G_i(x)\), where we care more about errors for instances with high weights.
		li Repeating the process
	p The final decision model is based on all the weak models..
	p \(G(x)=\sign \sum_{i=1}^n\alpha_i G_i(x)\)
	p If the sign is positive, we label \(1\), otherwise \(-1\).
	p High importance weak models identify important variables. This can be used for feature selection.
	h3
		b Bagging
	p Bagging is short for bootstrap aggregation.
	p We divide the training data into subsets, and train separately.
	p Aggregation takes the estimators from each of the bootstraps and averages them.
	p Random forests are a type of bagging method, for decision trees. We discuss these in more detail later.