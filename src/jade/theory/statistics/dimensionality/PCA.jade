extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Principal component analysis
block subSubContent
	h3
		b Introduction
	p Principal component analysis takes a dataset \(X\) with \(m\) variables and returns a principal component matrix \(A\) with size \(m\times k\).
	p Each new dimension is a linear function of the existing data. \(Z=XA\).

	p Each dimension in uncorrelated, and ordered, in order of descending explanation of variability.
	p The problem of principal component analysis is to find these weightings \(A\).
	h3
		b Classical PCA
	p We take the first \(k\) eigenvectors of the covariance matrix, ordered by eigenvalue.
	h3
		b Getting the eigenvectors using SVD
	p We can decompose \(X=U\Sigma A^T\).
	p We can take the eigenvectors from \(A\).
	h3
		b Choosing the number of dimension
	p We can choose \(k\) such that a certain percentage of the variance is retained.
	h3
		b Robust PCA
	p Robust PCA can be used to deal with corrupted data, such as corrupted image data.
	p Rather than data \(X\) we have \(M=L_0+S_0\) where \(L_0\) is what we want to recover (and is low rank), and \(S_0\) is noise (and sparce).	
	p In video footage, \(L_0\) can correspond to the background, while \(S_0\) corresponds to movement.
