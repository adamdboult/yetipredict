extends ./neuralNetworks/subSubTemplate.jade
block subSubContent
	h3
		b Multi-Layer Perceptron (MLP)
	p
		a(href="./neuralNetworks/hiddenLayers") A single hidden layer
	p
		a(href="./neuralNetworks/dummies") Dummies in neural networks
	p
		a(href="./neuralNetworks/backPropagation") Back propagation (section on training the dummies)
	p
		a(href="./neuralNetworks/illConditioning") Ill conditioning
	h3
		b More than \(2\) classes
	p
		a(href="./neuralNetworks/activation") Softmax
	p
		a(href="./neuralNetworks/LWTA") Local Winner Takes All (LWTA) layer
	p
		a(href="./neuralNetworks/maxout") Maxout layer
	h3
		b Regression
	h3
		b Deep neural networks
	p
		a(href="./neuralNetworks/deepLearning") More layers allow for more complex functions
	p
		a(href="./neuralNetworks/convexity") Convexity and local and global minima
	p
		a(href="./neuralNetworks/unstable") Unstable gradient problem
	p
		a(href="./neuralNetworks/curse") Curse of dimensionality
	h3
		b Regularising neural networks
	p
		a(href="./neuralNetworks/normalisation") Feature normalisation
	p
		a(href="./neuralNetworks/dropout") Dropout, and dropout layers
	p
		a(href="./neuralNetworks/l2") \(L_2\) regularisation (including how to change backprob algorithm)
	p
		a(href="./neuralNetworks/sparse") Sparse networks
	p
		a(href="./neuralNetworks/sharing") Parameter sharing
	p
		a(href="./neuralNetworks/decay") Weight decay
	p
		a(href="./neuralNetworks/anomoly") The anomoly detection problem
	p
		a(href="./neuralNetworks/earlyStopping") Early stopping
	p
		a(href="./neuralNetworks/residual") Residual blocks
	h3
		b Optimisation
	p
		a(href="./neuralNetworks/inputNorm") Input layer normalisation
	p
		a(href="./neuralNetworks/batchNorm") Batch normalisation
	h3
		b Alternatives to backpropagation
	p
		a(href="./neuralNetworks/greedyPretraining") Greedy pretraining
	p
		a(href="./neuralNetworks/cascade") Cascade-correlation learning architecture
	p
		a(href="./neuralNetworks/extreme") Extreme learning machines
	h3
		b Convolutional layers
	p
		a(href="./neuralNetworks/convolutional") Convolutional layers (Input + kernel = feature map. This is the convolution. Page on convolution operations?)
	p
		a(href="./neuralNetworks/convolutional") Invariance of convolutional layers (rotation, translation)
	p
		a(href="./neuralNetworks/flattening") Flattening layers
	p
		a(href="./neuralNetworks/multiScale") Multi-scale convolutions (invariance of zooming)
	h3
		b Pooling(max pooling, average pooling, subsampling)
	p
		a(href="./neuralNetworks/pooling") Pooling layers
	p
		a(href="./neuralNetworks/VLAD") Vector of Locally Aggregated Descriptors (VLAD)
	h3
		b Other window layers
	p
		a(href="./neuralNetworks/capsules") Capsules
	h3
		b Pre-training
	p
		a(href="./neuralNetworks/preTraining") Pre-training
	p
		a(href="./neuralNetworks/reset") Resetting parameters
	p
		a(href="./neuralNetworks/freeze") Freeze layers
	h3
		b Other
	p
		a(href="./neuralNetworks/sparsityRepresentation") Representational sparsity
	p
		a(href="./neuralNetworks/catastrophic") Catastrophic interference
	p
		a(href="./neuralNetworks/attention") Attention and Neural Turing Machines
	p
		a(href="./neuralNetworks/probabilistic") Probabilistic neural networks

