extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Hidden layers
block subSubContent
	h3
		b 
	p number of features can get out hand with linear and logistic stuff
	p eg x1x2x3, x1^2x2... is o(n3) for cubed, even higher otherwise

	p neural networks do non linear better
	p Why nn better than linear,
	p Universality theorem of single hidden layer
	p classification and regression
	p economics, interpreting stats?
	p Add study of softmax cost function



	p deeper models can compute more complex ideas
	p Can emulate logical gates
	h3
		b Defining a feed forward model
	p We refer to the value of a node as \(ai^(j)\), the activation of unit \(i\) in layer \(j\).
	p \(\Theta^(j)\) is a matrix of weights for mapping layer \(j\) to \(j+1\).
	
	p Input layer: number of inputs
	p Output layer: \(K\) classes
	p Hidden layers: \(L\) layers with \(S_l\) units in layer \(l\) - excluding the bias unit. A model can use just one hidden layer. Using twice as many nodes as inputs is a good starting point.

	h3
		b Calculating activitation
	p a=sig(z)
	p Each calculation includes a bias input.
	p Each node is a logit model. rather than pass to output, pass to a range of hidden layers first. We add a bias term, \(x_0=1\), for each node.
	p Each neuron will have different parameters.
	p Once a layer has been calculated, use activiation values for the next layer.

	p \(z_j^l =\sum_{}\theta_{ij}a_j^{l-1}\)
	p \(a_j^l =g(z_j^l)\)
	p Where \(g(z)\) is the sigmoid function.
	p 
