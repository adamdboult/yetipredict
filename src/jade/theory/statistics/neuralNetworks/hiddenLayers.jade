extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Hidden layers
block subSubContent
	h3
		b Introduction
	p In the perceptron we have input vector \(x\), and output:
	p \(a=a(wx)\)
	p We can augment the perceptron by adding a hidden layer.
	p Now the output on the activation function is an input to a second layer. By using different weights, we can create a second vector of inputs to the second layer.
	h3
		b The parameters of a feed forward model
	p \(\Theta^{j}\) is a matrix of weights for mapping layer \(j\) to \(j+1\). So we have \(\Theta^1\) and \(\Theta^2\).
	p If we have \(s\) units in the hidden layer, \(n\) features and \(k\) classes:
	ul
		li The dimension of \(\Theta^1\) is \((n+1) \times s\)
		li The dimension of \(\Theta^2\) is \((s+1) \times k\)
	p These include the offsets for each layer.
	h3
		b The activation function of a multi-layer perceptron
	p For a perceptron we had \(a=f(wx)\). Now we have:
	p \(a_i^j=f(a_{j-1}\Theta_{j-1})\)
	p We refer to the value of a node as \(a_i^{j}\), the activation of unit \(i\) in layer \(j\).

	h3
		b Initialising parameters
	p We start by randomly initialisng the value of each \(\theta \).
	p We do this to prevent each neuron from moving in sync.


