extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Delta rule
block subSubContent
	h3
		b Introduction
	p In machine learning we use gradient descent techniques to find parameters.
	p In a simple neural network, with no hidden nodes, we have an array \(x\) of inputs, and parameter weights of \(\theta \).
	p Therefore the activation on a neuron is:
	p \(z=\sum_x_i\theta_i\)
	p We have the link function \(a=a(h)\), and the actual correct value of \(y\).
	p The delta rule tells us to update the parameter \(\theta \), given an observed correct value.
	h3
		b The delta rule
	p The delta rule is:
	p \(\Delta \theta_i=\alpha \sum_j(y_j-a_j)a'(z_j)x_{ij}\)
	p We can also define \(\delta_i=-\frac{\delta E}{\delta z_j}=\sum_j(y_j-a_j)a'(z_j)\)
	p In this case:
	p \(\Delta \theta_i=\alpha \delta_j x_{ij}\)
	h3
		b Deriving the rule
	p The error of the network is:
	p \(E=\sum_j\frac{1}{2}(y_j-a_j)^2\)
	p We can see the change in error as we change the parameter:
	p \(\frac{\delta E}{\delta \theta_i }=\frac{\delta E}{\delta a_j}\frac{\delta a_j}{\delta z_j}\frac{\delta z_j}{\delta \theta_i}\)
	p \(\frac{\delta E}{\delta \theta_i }=-\sum_j(y_j-a_j)a'(z_j)x_{ij}\)
	
