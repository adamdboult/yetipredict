extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Extreme learning machines
block subSubContent
	h3
		b Introduction
	p This is an alternative to backprobagation for training a feedforward neural network.

	p We start with random parameters for each layer \(W_i\).

	p We have:

	p \(\hat y=W_2\sigma (W_1 x)\)

	p Etc.

	p We calculate:

	p \(W_2=\sigma(W_1x)^+Y\)

 

	p So W_1 is random and not updated.

	p W_2 is assigned to minimise loss, where \(W_2\) has no activation function.

