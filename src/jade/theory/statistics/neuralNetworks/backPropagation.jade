extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Back propagation
block subSubContent
	h3
		b Initialising
	p We start by randomly initialisng the value of each \(\theta \).
	p We do this to prevent each neuron from moving in sync.
	h3
		b Calculating errors for the last layer
	p For the last layer, the error is the difference between the activiation and actual label
	p The activitation is written as:
	p \(a_j^l\)
	p The error for example \(e\) is:
	p \(E_e(\theta )=\frac{1}{2}\sum_{k}(y_k-a_j^L)^2\)
	p We use gradient descent to change \(\theta \).
	p \(\Delta \theta_{ij}=-\alpha \frac{\delta E_e(\theta )}{\delta \theta_{ij}}\)
	p \(\Delta \theta_{ij}=-\alpha \frac{\delta E_e(\theta )}{\delta z_{j}^l}\frac{\delta z_j^l}{\delta \theta_{ij}}\)
	p \(\Delta \theta_{ij}=-\alpha \frac{\delta E_e(\theta )}{\delta z_{j}^l}a_i^l\)
	h3
		b Back propagation
	p Back propagation refers to errors being sent backwards from the ultimate layer.
	p For node \(j\) in layer \(l\) we can write the error as:
	p \(\delta_j^l\)
	
	p This is set to the partial derivative of the cost function with respect to to z(l)

	p For each layer there is a matrix, where the columns and rows represent the \(theta \) between the current layer and the next layer. We have a matrix for each layer in the network.
	h3
		b Checking
	p We can use a slower alternative to back propagation to ensure that the algorithm is working correctly.
	p Gradient checking is used to manually work out the gradient of the cost function.
	p We can do this by taking a value of theta, adding a small value, \(\epsilon \), to and from it to get \(\theta +\) and \(\theta -\).
	p We can then calculate the gradient for \(\theta_i\) by calculating:
	p \(\frac{J(\theta +)-J(\theta -)}{2\epsilon }\)
