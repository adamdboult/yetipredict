extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Forward propagation
block subSubContent
	h3
		b Defining a feed forward model
	p We refer to the value of a node as \(ai^(j)\), the activation of unit \(i\) in layer \(j\).
	p \(\Theta^(j)\) is a matrix of weights for mapping layer \(j\) to \(j+1\).
	
	p Input layer: number of inputs
	p Output layer: \(K\) classes
	p Hidden layers: \(L\) layers with \(S_l\) units in layer \(l\) - excluding the bias unit. A model can use just one hidden layer. Using twice as many nodes as inputs is a good starting point.

	h3
		b Calculating activitation
	p a=sig(z)
	p Each calculation includes a bias input.
	p Each node is a logit model. rather than pass to output, pass to a range of hidden layers first. We add a bias term, \(x_0=1\), for each node.
	p Each neuron will have different parameters.
	p Once a layer has been calculated, use activiation values for the next layer.

	p \(z_j^l =\sum_{}\theta_{ij}a_j^{l-1}\)
	p \(a_j^l =g(z_j^l)\)
	p Where \(g(z)\) is the sigmoid function.