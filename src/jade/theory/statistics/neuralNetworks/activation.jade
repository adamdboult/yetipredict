extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Types of activation functions
block subSubContent
	h3
		b Introduction
	h3
		b 1: Step
	p Used in perceptron
	p Not smooth. Differential is \(0\) at all points.
	h3
		b ReLU
	p ReLU, the Rectified Linear Unit, is defined as:
	p \(R(z)=\max (0,z)\)
	p Its differential is therefore 1 for values of \(z\) above \(0\), and \(0\) for values of \(z\) below \(0\).
	p The differential is undefined at \(z=0\), however this is unlikely to occur in practice.
	h3
		b Softplus
	p The softplus function is a smooth approximation of the ReLU function.
	p \(SP(z)=\ln (1+e^z)\)
	p Its derivative is the sigmoid function:
	p \(SP'(z)=\frac{1}{1+e^{-z}}\)
	h3
		b Sigmoid
	p The sigmoid activation function is:
	p \(\sigma (z)=\frac{1}{1+e^{-z}}\)
	p The differential of this function is:
	p \(\sigma '(z)=\frac{e^{-z}}{(1+e^{-z})^2}\)
	p \(\sigma '(z)=\sigma (z)\frac{1+e^{-z}-1}{1+e^{-z}}\)
	p \(\sigma '(z)=\sigma (z)[1-\sigma (z)]\)
	p The range of this activation is between 0 and 1.
	h3
		b Softmax
	p The softmax function is often used in the last layer of a classification network.
	p The softmax function is based on the sigmoid function. Whereas the sigmoid function flattens a single number to between \(0\) and \(1\), the softmax function does this for all inputs in a vector.
	p \(SM(z_j)=\frac{e^{z_j}}{\sum_{i}e^{z_i}}\)
