extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Types of activation functions
block subSubContent
	h3
		b Introduction
	p Each layer in a neural network takes the input vector \(x\) and multiplies it by a weight vector \(w\):
	p \(z=wx\)
	p We then apply an activation function to this:
	p \(a=f(z)\)
	p There are many options for activation functions.
	p As with the perceptron, we add an entry of \(1\) to the vector to allow for an intercept.
	h3
		b Step function
	p These is the activation function used in the perceptron.
	p If the sum is above \(0\), \(a(z)=1\). Otherwise, \(a(z)=0\).
	p This function is not smooth, and never has a differential other than \(0\).
	p Not smooth. Differential is \(0\) at all points.
	h3
		b Sigmoid
	p The sigmoid activation function is:
	p \(\sigma (z)=\frac{1}{1+e^{-z}}\)
	p The differential of this function is:
	p \(\sigma '(z)=\frac{e^{-z}}{(1+e^{-z})^2}\)
	p \(\sigma '(z)=\sigma (z)\frac{1+e^{-z}-1}{1+e^{-z}}\)
	p \(\sigma '(z)=\sigma (z)[1-\sigma (z)]\)
	p The range of this activation is between \(0\) and \(1\).
	h3
		b ReLU
	p ReLU, the Rectified Linear Unit, is defined as:
	p \(a(z)=\max (0,z)\)
	p Its differential is therefore \(1\) for values of \(z\) above \(0\), and \(0\) for values of \(z\) below \(0\).
	p The differential is undefined at \(z=0\), however this is unlikely to occur in practice.
	p The ReLU activation function induces sparcity.
	h3
		b Softplus
	p The softplus function is a smooth approximation of the ReLU function.
	p \(a(z)=\ln (1+e^z)\)
	p Its derivative is the sigmoid function:
	p \(a'(z)=\frac{1}{1+e^{-z}}\)
	p Unlike the ReLU function, Softplus does not induce sparcity.
	h3
		b Softmax
	p The softmax function is often used in the last layer of a classification network.
	p It takes a vector of dimension \(k\) and returns another vector of the same size. Only, this time all numbers are between \(0\) and \(1\) and the values sum to \(1\).
	p The softmax function is based on the sigmoid function.
	p \(a_j(z)=\frac{e^{z_j}}{\sum_{i}e^{z_i}}\)
s
