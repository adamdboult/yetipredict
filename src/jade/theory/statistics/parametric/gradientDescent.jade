extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Gradient descent
block subSubContent
	p There are \(3\) main algorithms for gradient descent:
	ul
		li Batch gradient descent
		li Stochastic gradient descent
		li Mini-batch gradient descent
	h3
		b Map reduce
	p Split data into sets
	p eg data into 4 sets, for 4 computers

	p each sums over their part of grad formula, these are then sent back and combined and divided over relevant m value (sum of all used in each comp)

	p Can my learning algorithm be expressed as sums over training set? if so, can be divided up to computers

	p map reduce can also be done on multi core machines
	p latency less important on multicore

	h3
		b Batch gradient descent
	p Do gradient descent on all samples
	p The standard gradient descent algorithm above is also known as batch gradient descent. There are other implementations.

	h3
		b Stochastic gradient descent
	p Do gradient descent on one (?!) sample only

	p not guaranteed for each step to go towards minimum, but each step much faster
	h3
		b Mini batch gradient descent
	p use b samples on each iteration, b is parameter, between stochastic and batch
	p b=2-100 for example




