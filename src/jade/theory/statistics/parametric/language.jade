extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Language models
block subSubContent
	h3
		b Naive Bayes
	p We can use naive bayes to predict the label of new text
	p y_new = argmax y\in Y p(y)\prod_j p(a_j |y)

	p estimate p(y) with frequency
	p estimate p(a|y)

	p we use m-estimate
	p p(a|y) = frac{n_c + m * p}{n_y + m}
	p where n_y is number of examples where class is y
	p n_c is number of examples where class is y and feature x is a
	p uniform prior: p = 1/k

	p we assume independence of word position

	p specifically for text we do:
	p p(w_k|c_j)=\frac{n_k +1}{n_j + |voc|}
	p where n_j is the number of word positions found in the text of class j 
	p n_k is the number of time w_k is found amongst these n_j word positions

	p this learns the probability that a random word chosen from class j is word k

	p we also learn p(c_j) = \frac{|docs_j|}{|examples|}
	h3
		b probabilistic language model
	p probabilistic language model:
	p p(child|did you call your)

	p can estimate p(child|did you call your) = \frac{|did you call your child|}{|did you call your|}
	p can estimate p(did you call your child) = \frac{|did you call your child|}{|sentences 5 words|}
	p need large corpus

	p we can modify these
	p can estimate p(child|did you call your) = \frac{|did you call your child| + 1}{|did you call your| + V}
	p can estimate p(did you call your child) = \frac{|did you call your child| + 1}{|sentences 5 words| + V}
	p called smoothing

	p decompose using chain rule
	p p(w_1, w_2, ...,w_5) = p(w_1)p(w_2|w_1)...p(w_5|w_4, w_3, w_2, w_1)
	p p(w_1, w_2, ...,w_5) = \prod_k p(w_k|w_1,...,w_k-1)

	p markov assumptionL p(w_k|w_1,...,w_k-1)=p(w_k|w_k-1)
	p other models, look n-1 back: n-gram

	p we can estiamte these conditional prob using the count model above.

	p can compare models using Perplexity
	p perplexity(w_1, w_2, ..., w_n)=p(w_1, w_2, ..., w_n)^{-\frac{1}{n}}
	p we do this for words in corpus

	p we can expand it
	p perplexity(w_1, w_2, ..., w_n)=(\prod p(w_i| w_1, ..., w_{i-1})^{-\frac{1}{n}})
	p depending on which n-gram we use we can then simplify this.

	p choose model with lowest perplexity
