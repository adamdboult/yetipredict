extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Delta rule
block subSubContent
	h3
		b Introduction
	p We want to train the parameters \(\theta \).
	p We can do this with gradient descent, by working out how much the loss function falls as we change each parameter.
	p The delta rule tells us how to do this.
	h3
		b The loss function
	p The error of the network is:
	p \(E=\sum_j\frac{1}{2}(y_j-a_j)^2\)
	p We know that \(a_j=a(\theta x_j)\) and so:
	p \(E=\sum_j\frac{1}{2}(y_j-a(\theta x_j))^2\)
	h3
		b Minimising loss
	p We can see the change in error as we change the parameter:
	p \(\frac{\delta E}{\delta \theta_i }=\sum_j \frac{\delta E}{\delta a_j}\frac{\delta a_j}{\delta z_j}\frac{\delta z_j}{\delta \theta_i}\)
	p \(\frac{\delta E}{\delta \theta_i }=-\sum_j(y_j-a_j)a'(z_j)x_{ij}\)
	h3
		b Delta
	p We define delta as:
	p \(\delta_i=-\frac{\delta E}{\delta z_j}=\sum_j(y_j-a_j)a'(z_j)\)
	p So:
	p \(\frac{\delta E}{\delta \theta_i }=\delta_i x_{ij}\)

	h3
		b The delta rule
	p We update the parameters using gradient descent:
	p \(\Delta \theta_i=\alpha \delta_i x_{ij}\)

