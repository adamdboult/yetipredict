extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Bayes theorem
block subSubContent
	h3
		b Conditional probability
	p We define conditional probability
	p $$P(E_i|E_j):=\frac{P(E_i\land E_j)}{P(E_j)}$$
	p We can show this is between \(0\) and \(1\).
	p $$P(E_j)=P(E_i\land E_j)+P(\bar{E_i}\land E_j)$$
	p $$P(E_i|E_j):=\frac{P(E_i\land E_j)}{ P(E_i\land E_j)+P(\bar{E_i}\land E_j)}$$

	p We know:
	p $$P(x_i|y_j):=\frac{P(x_i \land y_j)}{P(y_j)}$$
	p $$P(y_j|x_i):=\frac{P(x_i \land y_j)}{P(x_i)}$$
	p So:
	p $$P(x_i|y_j)P(y_j)=P(y_j|x_i) P(x_i)$$
	p $$P(x_i|y_j)=\frac{P(y_j|x_i) P(x_i)}{P(y_j)}$$
	p Note that this is undefined when \(P(y_j)=0\)


	p Note that for the same event, 
	p $$P(x_i|x_j)=\frac{P(x_i\land x_j)}{P(x_j)}$$
	p $$P(x_i|x_j)=0$$

	p For the same outcome:
	p $$P(x_i|x_i)=\frac{P(x_i\land x_i)}{P(x_i)}$$
	p $$P(x_i|x_i)=\frac{P(x_i)}{P(x_i)}$$
	p $$P(x_i|x_i)=1$$

	h3
		b Bayes theorem
	p From the definition of conditional probability we know that:
	p $$P(E_i|E_j):=\frac{P(E_i\land E_j)}{P(E_j)}$$
	p $$P(E_j|E_i):=\frac{P(E_i\land E_j)}{P(E_i)}$$
	p So:
	p $$P(E_i\land E_j)=P(E_i|E_j)P(E_j)$$
	p $$P(E_i\land E_j)=P(E_j|E_i)P(E_i)$$
	p So:
	p $$P(E_i|E_j)P(E_j)=P(E_j|E_i)P(E_i)$$
	h3
		b Independent variables
	p Events are independent if:
	p $$P(E_i|E_j)=P(E_i)$$
	p Note that:
	p $$P(E_i\land E_j)=P(E_i|E_j)P(E_j)$$
	p And so for independent events:
	p $$P(E_i\land E_j)=P(E_i)P(E_j)$$

	h3
		b Conjugate priors
	p If the prior \(P(\theta)\) and the posterior \(P(\theta | X)\) are in the same family of distributions (eg both Gaussian), then the prior and posterior are conjugate distributions
	