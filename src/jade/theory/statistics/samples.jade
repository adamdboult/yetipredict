extends ../../templates/theoryHeader.jade

block theoryContent
	h2
		b Moments
	h3
		b Samples
	p
		b Sample set
	p We only observe the sample set \(S\), drawn from \(n\) random event spaces, not the probability functions themselves. It is the nature of the probability functions we are interested in.
	p For example we could observe a sample set from flipping a coin of:
	p $$\{H,T,T,H\}$$
	p We could also have more complex sets. For example if you were analysing temperature and time of day this could look like:
	p $$\{24,13:00,17,14:00, 19,15:00\}$$
	p
		b Sample measures
	p We can take measures on the sample and probability. These can enable us to estimate probability measures.
	p We previously discussed taking measures on the population distribution through calculating moments. 
	p We can also take measures on the sample set. Options include the sample size, the sample mean and the sample variance.
	p $$n:=count(x_i)$$
	p We can use similar thing to moments. The \(m\)th sample moment of \(x\) around \(c\) is:
	p $$\:=\frac{\sum_i (x_i-c)^m}{n}$$
	p $$\bar x :=\frac{\sum_i x_i}{n}$$
	p $$s^2:=\frac{\sum_i (x_i - \bar x)^2}{n}$$
	p E(\bar x)=E(x)
	p E(s^2)/=E(x-mew)^2
	p Weak law of large numbers
	p Let’s compare the sample average to the expected value. The sample average is:
	p $$\bar x =\frac{\sum_j x_j}{n}$$
	p The expected value is:
	p $$E(x)=\sum_i x_i P(x_i)$$
	p If \(q_i\) is the number of observations for that possible outcome, we know that:
	p $$\sum_j x_j =\sum_i x_i q_i$$
	p And so:
	p $$\bar x =\frac{\sum_j x_j}{n}=\frac{\sum_i x_i q_i}{n}$$
	p $$\bar x =\frac{\sum_j x_j}{n}=\sum_i x_i \frac{q_i}{n}$$
	p What happens to \(\frac{q_i}{n}\) as \(n\) increases?
	p Population estimates from samples
	p As n to infinity, looks like population

	p Small sample best estimate of population moments, has error ranges

	p Other sample stuff
	p Strong law of large numbers

	p Central limit theorem





	p Estimators
	p Where \(i\ne j\):
	p $$E[(x_i-x_j)^2]=E[x_i^2+x_j^2-2x_ix_j]$$
	p $$E[(x_i-x_j)^2]=E[x_i^2]+E[x_j^2]-2E[x_ix_j]$$
	p $$E[(x_i-x_j)^2]=[E(x)^2+Var(x)]+[E(x)^2+Var(x)]-2E(x)^2$$
	p $$E[(x_i-x_j)^2]=2Var(x)$$
	p $$Var(\bar x)=\Var(\frac{x_1+x_2+x_3…}{n})$$
	p $$Var(\bar x)=\frac{1}{n^2}Var(x_1+x_2+x_3…)$$
	p $$Var(\bar x)=\frac{1}{n^2}\{\sum^n_i \sum^n_j Cov(x_i, x_j)\}$$
	p $$Var(\bar x)=\frac{1}{n^2}\{[\sum^n_i Var(x_i)]+[\sum_i \sum_{j\ne i} Cov(x_i, x_j)\}$$
	p If \(x\) is identically distributed then \(Var(x_i)=Var(x)\):
	p $$Var(\bar x)=\frac{1}{n^2}\{[nVar(x)]+[\sum_i \sum_{j\ne i} Cov(x_i, x_j)\}$$
	p If \(x\) is independently distributed then \(Cov(x_i,x_{j\ne i})=0\):
	p $$Var(\bar x)=\frac{1}{n^2}\{[nVar(x)]$$
	p $$Var(\bar x)=\frac{Var(x)}{n}$$
	
	p $$E[s^2]=E[\frac{\sum_i(x_i-\bar x)^2}{n}]$$
