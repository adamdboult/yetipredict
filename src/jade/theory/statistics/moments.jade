extends ../../templates/theoryHeader.jade

block theoryContent
	h2
		b Moments
	h3
		b Moments and expected values
	p
		b Expected value
	p For a random variable (or vector of random variables), \(x\), we define the expected value of \(f(x)\) as :
	p $$E[f(x)]:=\sum f(x_i) P(x_i)$$
	p The expected value of random variable \(x\) is therefore this where \(f(x)=x\).
	p $$E(x)=\sum_i x_i P(x_i)$$
	p We can also take moments of multiple random variables. For this we see \(f(x)\) as a function on a vector \(x\), and \(P(x_i)\) as the probability of a specific vector appearing:
	p $$E[f(x,y)]=\sum_i \sum_j f(x_i,y_j) P(x_i \land y_j)$$
	p
		b Properties of expected values
	p We can show that \(E(x+y)=E(x)+E(y)\):
	p $$E[x+y]=\sum_i \sum_j (x_i+y_j) P(x_i \land y_j)$$
	p $$E[x+y]=\sum_i \sum_j x_i [P(x_i \land y_j)]+\sum_i \sum_j [y_j P(x_i \land y_j)]$$
	p $$E[x+y]=\sum_i x_i \sum_j [P(x_i \land y_j)]+\sum_j y_j \sum_i [P(x_i \land y_j)]$$
	p $$E[x+y]=\sum_i x_i P(x_i)+\sum_j y_j P(y_j) $$
	p $$E[x+y]=E[x]+E[y]$$
	p
		b Moments
	p The \(n\)th moment of event \(x\) around \(c\) is defined as this where \(f(x)=(x-c)^n\)
	p $$E[(x-c)^n]=\sum_i (x_i-c)^n P(x_i)$$
	p The mean is the first moment around \(0\).
	p The variance is the second moment around the mean.
	p Variance and e(x)^2, e(x^2)
	p Var(x+y)=




	p More stats
	p Expectations
	p $$E(cx)=\sum_i cx P(x_i)$$
	p $$E(cx)=c\sum_i x P(x_i)$$
	p $$E(cx)=cE(x)$$

	p $$E(c)=\sum_i c_i P(c_i)$$
	p $$E(c)= c P(c)$$
	p $$E(c)= c$$
	p Variance
	p $$Var(x)=E((x-E(x))^2)$$
	p $$Var(x)=E(x^2+E(x)^2-2xE(x))$$
	p $$Var(x)=E(x^2)+E(E(x)^2)-E(2xE(x))$$
	p $$Var(x)=E(x^2)+E(x)^2-2E(x)^2$$
	p $$Var(x)=E(x^2)-E(x)^2$$

	p $$Var(c)=E(c^2)-E(c)^2$$
	p $$Var(c)= c^2-c^2$$
	p $$Var(c)=0$$

	p $$Var(cx)=E((cx)^2)-E(cx)^2$$
	p $$Var(cx)=E(c^2x^2)-[\sum_i cx P(x_i)]^2$$
	p $$Var(cx)=c^2E(x^2)-c^2[\sum_i x P(x_i)]^2$$
	p $$Var(cx)=c^2[E(x^2)- E(x)^2]$$
	p $$Var(cx)=c^2Var(x)$$

	p $$Var(x+y)=E((x+y)^2)-E(x+y)^2$$
	p $$Var(x+y)=E(x^2+y^2+2xy)-E(x+y)^2$$
	p $$Var(x+y)=E(x^2)+E(y^2)+E(2xy)-E(x+y)^2$$
	p $$Var(x+y)=E(x^2)+E(y^2)+E(2xy)-[E(x)+E(y)]^2$$
	p $$Var(x+y)=E(x^2)+E(y^2)+E(2xy)-E(x)^2-E(y)^2-2E(x)E(y)]$$
	p $$Var(x+y)=[E(x^2)-E(x)^2]+[E(y^2)-E(y)^2]+E(2xy)-2E(x)E(y)$$
	p $$Var(x+y)=Var(x) +Var(y)+2[E(xy)-E(x)E(y)]$$
	p We then define:
	p $$Cov(x,y):=E(xy)-E(x)E(y)$$
	p Noting that:
	p $$Cov(x,x)=E(xx)-E(x)E(x)$$
	p $$Cov(x,x)=Var(x) $$
	p So:
	p $$Var(x+y)=Var(x)+Var(y)+2Cov(x,y)$$
	p $$Var(x+y)=Cov(x,x)+Cov(x,y)+Cov(y,x)+Cov(y,y) $$

	p $$Cov(x,c)=E(xc)-E(x)E(c)$$
	p $$Cov(x,c)=cE(x)-cE(x)$$
	p $$Cov(x,c)=0$$

	p $$Var(x+c)=Var(x) +Var(c)+2Cov(x,c)$$
	p $$Var(x+c)=Var(x)+0+0$$
	p $$Var(x+c)=Var(x)$$

	p $$E(x)^2+Var(x)=E(x)^2+E((x-E(x))^2)$$
	p $$E(x)^2+Var(x)=E(x)^2+E(x^2+E(x)^2-2xE(x))$$
	p $$E(x)^2+Var(x)=E(x)^2+E(x^2)+E(E(x)^2)-E(2xE(x))$$
	p $$E(x)^2+Var(x)=E(x)^2+E(x^2)+E(x)^2-2E(x)E(x))$$
	p $$E(x)^2+Var(x)=E(x^2)$$

	h3
		b Examples of distributions
	p We know:
	p $$\sum_i P(x_i)=1$$
	p This can be satisfied in many ways:
	ul
		li Only one possible outcome, with certainty
		li Six outcomes, equally likely
		li Two sides of a coin, but biased
		li The normal distribution
	p
		b Cumulative distributions
	p
		b Examples of joint distributions
	p These could include:
	ul
		li Rolling a die then flipping a coin
		li Flipping a coin, and then reading it again
		li Flipping a coin and then reading the other side
	p In the first example, these are independent, so we can model:
	p $$P(x_i \land y_j)=P(x_i).P(y_j)$$
	p In the second these are the same measurement so:
	p $$P(x_i \land y_j)=P(x_i \land x_i)=P(x_i)$$
	p The third example is mutually exclusive so:
	p $$P(x_i \land y_j)=P(x_i \land x_{¬j})$$
	p Which is either \(0\) or \(P(x_i)\) depending on \(j\).
	p We can try more complicated examples, for example flipping the same coin multiple times.
	p $$P(x_i \land y_j \land z_k …)=P(x_i \land x_j \land x_k …)=P(x_i).P(x_j).P(x_z)…$$


	h3
		b Some theorems
	p
		b Markov’s inequality
	p
		b Chebyshev's inequality


