extends ./subSubTemplate.jade
block subSubTitle
	h1
		b \(L_p\) regularisation
block subSubContent
	h3
		b Introduction
	p We can generalise this to:
	p \(w_{l_p} = \arg \min ||y-Xw||^2_2+\lambda ||w||^p_p\)
	p For ridge regression there is always a solution.
	p For least squares there is a solution if X^TX is invertible
	p For Lasso we must use numerical optimisation.
	p lasso and L1 induces sparcity

	p goal is min ||y - f(x)|| + \lambda g(w)
	p ridge regression: g(w)=||w||
	p if lambda 0, OLS, if infinite, w goes to 0

	p normal equation changes to: (\lambda I + X^TX)^{-1}X^Ty
	p we can preprocess to avoid processing of 1s. shift mean of y to 0. normalise x mean 0 var 1


