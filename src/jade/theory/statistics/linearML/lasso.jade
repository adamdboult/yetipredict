extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Lasso
block subSubContent
	h3
		b Introduction
	p Regularisation of LLS. Sum of thetas are constrained to be below hyperparameter \(t\)
	p L1 regularisation
	p This is also known as sparce regression, because many weights are set to \(0\).
	p This now looks like:
	p \(w_{lasso} = \arg \min ||y-Xw||^2_2+\lambda ||w||_1\)
	h3
		b \(l_p\) regression
	p We can generalise this to:
	p \(w_{l_p} = \arg \min ||y-Xw||^2_2+\lambda ||w||^p_p\)
	p For ridge regression there is always a solution.
	p For least squares there is a solution if X^TX is invertible
	p For Lasso we must use numerical optimisation.
