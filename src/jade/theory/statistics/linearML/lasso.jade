extends ./subSubTemplate.jade
block subSubTitle
	h1
		b Lasso
block subSubContent
	h3
		b Introduction
	p Regularisation of LLS. Sum of thetas are constrained to be below hyperparameter \(t\)
	p L1 regularisation
	p This is also known as sparce regression, because many weights are set to \(0\).
	p This now looks like:
	p \(w_{lasso} = \arg \min ||y-Xw||^2_2+\lambda ||w||_1\)
	h3
		b \(l_p\) regression
	p We can generalise this to:
	p \(w_{l_p} = \arg \min ||y-Xw||^2_2+\lambda ||w||^p_p\)
	p For ridge regression there is always a solution.
	p For least squares there is a solution if X^TX is invertible
	p For Lasso we must use numerical optimisation.
	h3
		b tikhinov regularisation

	p lasso and L1 induces sparcity

	p goal is min ||y - f(x)|| + \lambda g(w)
	p ridge regression: g(w)=||w||
	p if lambda 0, OLS, if infinite, w goes to 0

	p normal equation changes to: (\lambda I + X^TX)^{-1}X^Ty
	p we can preprocess to avoid processing of 1s. shift mean of y to 0. normalise x mean 0 var 1

	p maximum a priori estimation. equiv to ridge regression with a priori estimate of 0

	p W_{RR}=(\lamda I+X^TX)^{-1}X^Ty
	p E_[w{RR}]=(\lambda I+X^TX)^{-1}X^TXw
	p Var[W_{RR}]=\a

	h3
		b Maximum A-Priori (MAP) estimator for linear regression

