extends ./optimisation/subSubTemplate.jade
block subSubContent
	h3
		b Gradient descent
	p
		a(href="./optimisation/gradientDescent") Gradient descent
	p
		a(href="./optimisation/gradientDescentData") Batch, mini-batch, stochastic gradient descent
	p
		a(href="./optimisation/gradientDescentVariants") Momentum gradient descent
	p
		a(href="./optimisation/gradientDescentLearning") Adaptive learning rates (Adagrad, Adadelta, RMSProp, ADAptive Momentum (ADAM))
	h3
		b Differentiable on a single axis
	p
		a(href="./optimisation/coordinate") Coordinate descent
	h3
		b Twice-differentiable functions
	p
		a(href="./optimisation/newton") Newton methods
	p
		a(href="./optimisation/BFGS") BFGS
	h3
		b Non-differentiable functions
	p
		a(href="./optimisation/subGradient") Subgradient descent

