extends ./subSubTemplate.jade

block subSubTitle
	h1
		b Descent algorithms

block subSubContent
	h3
		b What is gradient descent?
	p Rather than solve a normal equation, gradient descent takes the loss function, and takes the derivative of the loss function with respect to each parameter.
	p Small adjustments are then made to the parameters, in the direction of the steepest derivative, resulting in better parameters.

	p As derivative term gets smaller, convergance happens. The largest changes to the parametres occurs early on in the algorithm.


	p can stop if not lowering by much


	h3
		b Gradient descent and feature scaling
	p makes trajectory smoother, gets there faster
	p get things to -1<x<1 range

	h3
		b Local minima
	p Gradient descent is not guaratneed to arrive at a global minimum. For some loss functions, there will be multiple local minima, and gradient descent can end up in the wrong one.
	p Linear regression does not have this issue.
	p As a result, when we create functions with loss functions, convextity is very important. If the loss space is convex, then we will not get stuck in a local minima.
	h3
		b Batch gradient descent
	p := used to denote an update of variable. Used in programming, eg x=x+1.
	p $$\theta _j := \alpha \frac{\delta }{\delta \theta _j}J(\theta _0,\theta _1)$$
	p \(\alpha \) sets rate of descent.



	p theta 0 := theta 0 - alpha/m sum(h0(x) - y)
	p theta j := theta j - alpha/m sum(h0(x) - y)xj

	p can check if j theta increasing, means bad methodology, lower alpha


	p get run for x iterations,evaluate j(theta)

	p Can use matrices to do each step


	p can check convergence by checking cost over last 1000 or so, rather than all

	p smaller learning rate can get to better solution, as can circle drain for small samples

	p slowly decreasing learning rate can get better solutions
	p alpha = const1/(i + cost2)

	h3
		b Introduction
	p There are \(3\) main algorithms for gradient descent:
	ul
		li Batch gradient descent
		li Stochastic gradient descent
		li Mini-batch gradient descent
	h3
		b Batch gradient descent
	p Do gradient descent on all samples
	p The standard gradient descent algorithm above is also known as batch gradient descent. There are other implementations.

	h3
		b Stochastic gradient descent
	p Do gradient descent on one (?!) sample only

	p not guaranteed for each step to go towards minimum, but each step much faster
	h3
		b Mini batch gradient descent
	p use b samples on each iteration, b is parameter, between stochastic and batch
	p b=2-100 for example




	h3
		b Epochs
	p This refers to the number of times the whole dataset has been run.
	p 
	h3
		b Checking
	p We can use a slower alternative to back propagation to ensure that the algorithm is working correctly.
	p Gradient checking is used to manually work out the gradient of the cost function.
	p We can do this by taking a value of theta, adding a small value, \(\epsilon \), to and from it to get \(\theta +\) and \(\theta -\).
	p We can then calculate the gradient for \(\theta_i\) by calculating:
	p \(\frac{J(\theta +)-J(\theta -)}{2\epsilon }\)

	h3
		b Limitations of normal equation
	p The normal equation may not always be appropriate. Solving it can be computationally very expensive, especially if there are a large number of features.
	p At around \(10000\) features, gradient descent can become faster than the normal equation.

	p normal needs no alpha, iterations, is exact


	p Derivative term for least squares:
	p $$\frac{\delta }{\delta \theta _0}J(\theta _0,\theta _1)=\frac{1}{m}\sum\lim_{i=1}^m (h_{\theta} (x^{(i)})-y^{(i)})$$
	p $$\frac{\delta }{\delta \theta _1}J(\theta _0,\theta _1)=\frac{1}{m}\sum\lim_{i=1}^m (h_{\theta} (x^{(i)})-y^{(i)})x^{(i)}$$
	p This is batch gradient descent. Each step of descent uses all training examples


