extends ./subSubTemplate.jade

block subSubContent
	h2
		b Determinants
	p A matrix can only be inverted if it can be created from a combination of elementary row operations.
	p How can we identify if a matrix is invertible? We want to create a scalar from the matrix which tells us if this possible.
	p For a matrix \(A\) we label this scalar, the determiannt, \(|A|\). We proposed \(|A|=0\) when the matrix is not invertible.
	p So how can we identify the function we need to undertake on the matrix?
	p We call this function the determinant. We show this through:
	p $$D(A)$$


	h3
		b Maps and forms


	p
		b Linear maps
	p A linear map (or function) is a map from one input to an output which preserves addition and scalar multiplication.
	p That is if function \(f\) is linear then:
	p $$f(aM+bN)=af(M)+bf(N)$$
	p Linear form
	p A linear form is a special type of linear map, which takes a vector and returns a scalar.
	p Let’s consider what this implies.
	p $$v=[a_1,…,a_n]$$
	p $$v=[a_1,…,0]+...+[0,…,a_n]$$
	p $$f(v)=f([a_1,…,a_n])
	p $$f(v)=f([a_1,…,0]+...+[0,…,a_n])$$
	p Because a linear map is linear in additon:
	p $$f(v)=f([a_1,…,0])+...+f([0,…,a_n])$$
	p $$f(v)=f(a_1[1,…,0])+...+f(a_n[0,…,1])$$
	p Because a linear map is linear in scaling:
	p $$f(v)=a_1f([1,…,0])+...+a_nf([0,…,1])$$
	p Because the function terms are now independent of the vector:
	p $$f(v)=a_1f_1+...+a_nf_n$$
	p $$f(v)=fv$$
	p That is, the linear form is a vector multiplication.
	p
		b Biliear map
	p A bilinear map (or function) is a map from two inputs to an output which preserves addition and scalar multiplication. This is in contrast to a linear map, which only has one input.
	p In addition, the function is linear in both arguments.
	p That is if function \(f\) is bilinear then:
	p $$X=aM+bN$$
	p $$Y=cO+dP$$
	p $$f(X,Y)=f(aM+bN,cO+dP)$$
	p $$f(X,Y)=f(aM,cO+dP)+f(bN,cO+dP)$$
	p $$f(X,Y)=f(aM,cO)+f(aM,dP)+f(bN,cO)+f(bN,dP)$$
	p $$f(X,Y)=acf(M,O)+adf(M,P)+bcf(N,O)+bdf(N,P)$$
	p Note that:
	p $$f(X,Y)=f(X+0,Y)$$
	p $$f(X,Y)=f(X,Y)+f(0,Y)$$
	p $$(0,Y)=0$$
	p That is, if any input is \(0\) in an additative sense, the value of the map must be zero.
	p
		b Bilinear form
	p A bilinear form is a special type of bilinear map, which takes a vector and returns a scalar.
	p
		b Multilinear map
	p This extends from \(2\) inputs to an arbitrary number of inputs.
	p
		b Multilinear form
	p A special type of multilinear map which takes vectors and returns a scalar.
	p $$f(M)=f([v_1,…,v_n])$$
	p We introduce \(e_i\), the element vector. This is \(0\) for all entries except for \(i\) where it is \(1\). Any vector can be shown as a sum of these vectors multiplied by a scalar.
	p $$f(M)=f([\sum^m_{i=1}v_{1i}e_i,…,\sum^m_{i=1}v_{ni}e_i])$$
	p $$f(M)=\sum^m_{k_1=1}f([v_{1k_1}e_{k_1},…,\sum^m_{i=1}v_{ni}e_i])$$
	p $$f(M)=\sum^m_{k_1=1}...\sum^m_{k_n=1}f([v_{1k_1}e_{k_1},…,v_{nk_n}e_{k_n}])$$
	p Because this in linear in scalars:
	p $$f(M)=\sum^m_{k_1=1}...\sum^m_{k_n=1}\prod_{j=1}^nv_{jk_j}f([e_{k_1},…,e_{k_n}])$$


	h3
		b Terminology
	p
		b Matrices as columns
	p A matrix can be shown in terms of its columns.
	p $$A=[a_1,a_2,...,a_n]$$
	p
		b Columns as sums of elements
	p An element vector \(E^k\) is empty except for \(1\) in row \(k\). It is the \(i\)th column of the identity matrix.
	p Column \(a_i\) can be shown as;
	p $$a_i=\sum_{j=1}^na_{ji}E^j$$
	h3
		b Desired properties
	p
		b Linear dependence
	p If a column is a linear combination of other columns, then the matrix cannot be inverted.
	p If:
	p $$A=[a_1,a_2,...,\sum_{j\ne i}^{n}c_ja_j,...,a_n]$$
	p $$D(A)=0$$

	p $$D(A)=D([a_1,a_2,...,\sum_{j\ne i}^{n}c_ja_j,...,a_n])=0$$

	h3
		b Rule 1: Multicolinearity
	p $$M=[a_1,a_2,...,\sum_{i=1}^{m}c_ib_i,...,a_n]$$
	p $$D(M)=D([a_1,a_2,...,\sum_{i=1}^{m}c_ib_i,...,a_n])$$
	p The following is the rule
	p $$D(M)=\sum_{i=1}^{m}c_iD([a_1,a_2,...,b_i,...,a_n])$$

	p
		b Result 1: A matrix with a column of \(0\)s has a determinant of \(0\)
	p Linear dependence implies that if a matrix has a column of \(0\)s then the determinant is \(0\).
	p $$A=[a_1,...,0,...,a_n]$$
	p $$D(A)=D([a_1,...,0,...,a_n])$$
	p $$D(A)=D([a_1,...,0.c,...,a_n])$$
	p $$D(A)=0.D([a_1,...,c,...,a_n])$$
	p $$D(A)=0$$$
	p
		b Result 2: Multiplying a column by \(-1\) multiplies the determinant by \(-1\)
	p Reversing the sign of a column reverses the sign of the determinant.
	p $$M_1=[a_1,a_2,...,a_i,...,a_n]$$
	p $$M_2=[a_1,a_2,...,-a_i,...,a_n]$$
	p $$D(M_2)=D([a_1,a_2,...,-a_i,...,a_n])$$
	p $$D(M_2)=D([a_1,a_2,...,(-1)(a_i),...,a_n])$$
	p Multiplying by a scalar multiplies the determinant, from Rule \(1\).
	p $$D(M_2)=(-1)D([a_1,a_2,...,a_i,...,a_n])$$
	p $$D(M_2)=-D([a_1,a_2,...,a_i,...,a_n])$$
	p $$D(M_2)=-D(M_1)$$

	p
		b Result 3: Swapping columns multiplies the determinant by \(-1\)
	p 
	p $$A=[a_1,...,a_i+a_j,...,a_i+a_j,...,a_n]$$
	p We know.
	p $$D(A)=0$$
	p $$D(A)=D([a_1,...,a_i,...,a_i,...,a_n])+D([a_1,...,a_i,...,a_j,...,a_n])+D([a_1,...,a_j,...,a_i,...,a_n])+D([a_1,...,a_j,...,a_j,...,a_n])=0$$
	p As \(2\) of these have equal columns these are equal to \(0\).
	p $$D([a_1,...,a_i,...,a_j,...,a_n])+D([a_1,...,a_j,...,a_i,...,a_n])=0$$
	p $$D([a_1,...,a_i,...,a_j,...,a_n])=-D([a_1,...,a_j,...,a_i,...,a_n])$$
	p
		b Result 4: A matrix with equal columns has a determinant of \(0\).
	p $$A=[a_1,...,a_i,...,a_i,...,a_n]$$
	p $$D(A)=D([a_1,...,a_i,...,a_i,...,a_n])$$
	p We know from Result 3 that swapping columns reverses the sign. Reversing columns results in the same matrix, so the determinant must be unchanged.
	p $$D(A)=-D(A)$$
	p $$D(A)=0$$

	p
		b Result 5: Linear dependence results in a determinant of \(0\).
	p Consider linearly dependent matrix \(A\):
	p $$A=[a_1,a_2,...,\sum_{j\ne i}^{n}c_ja_j,...,a_n]$$
	p $$D(A)=D([a_1,a_2,...,\sum_{j\ne i}^{n}c_ja_j,...,a_n])$$
	p $$D(A)=\sum_{j\ne i}^{n}c_jD([a_1,a_2,...,a_j,...,a_n])$$
	p From Result 4 we know that each of these determinants is \(0\) and therefore the outcome is \(0\).
	h3
		b Rule 2: Normalisation
	p We set \(D(I)=1\).

	h3
		b Calculating the determinant
	p
		b Matrix to element form
	p We have
	p $$A=[a_1,...,a_i,...,a_n]$$
	p Let's show this in element form.
	p $$A=[\sum_{j=1}^na_{j1}E^j,...,\sum_{j=1}^na_{ji}E^j,...,\sum_{j=1}^na_{jn}E^j]$$
	p The determinant of this is:
	p $$D(A)=D([\sum_{j=1}^na_{j1}E^j,...,\sum_{j=1}^na_{ji}E^j,...,\sum_{j=1}^na_{jn}E^j])$$
	p As this is a sum we can expand it out. As each addition creates \(2\) splits, we are summing across each element. That is there are \(n^2\) additions and \(n\) summation functions.


	p $$D(A)=\sum_{k=1}^nD([a_{k1}E^k,...,\sum_{j=1}^na_{ji}E^j,...,\sum_{j=1}^na_{jn}E^j])$$


	p $$D(A)=\sum_{k_1=1}^n...\sum_{k_n=1}^nD([a_{k_11}E^{k_1},...,a_{k_ii}E^{k_i},...,a_{k_kk}E^{k_n}])$$
	p $$D(A)=\sum_{k_1=1}^n...\sum_{k_n=1}^n(\prod_{l=1}^{n}a_{k_ll})D([E^{k_1},...,E^{k_i},...,E^{k_n}])$$
	p So what is the value of the determinant here?
	p We know that the determinant of the identity matrix is \(1\).
	p We know that the determinant of a matrix with identical columns is \(0\).
	p We know that swapping columns multiplies the determinant by \(-1\).
	p Therefore the determinants where the values of \(k\) are not all unique are \(0\).
	p The determinants of the others are either \(-1\) or \(1\) depending on how many swaps are required to restore to the identity matrix.
	p This is also shown as the Leibni formula.
	p $$\sum_{\sigma \in S_n}sgn (\sigma )\prod_{i=1}^na_{i,\sigma_i}$$

	h3
		b Properties of determinants
	p We know the below as it is of bilinear form.
	p
		b Multiplication
	p detAdetB=detAB
		
